{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FloydHub Introduction to Deep Learning: PyTorch\n",
    "\n",
    "<img style=\"align: center;height: 300px; widht: 300px;\" src=\"https://github.com/sominwadhwa/sominwadhwa.github.io/blob/master/assets/intro_to_pytorch_series/PyTorch.jpg?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[PyTorch](http://pytorch.org/) is one among the numerous [Deep Learning frameworks](https://www.kdnuggets.com/2017/02/python-deep-learning-frameworks-overview.html) which allows us to build powerful Deep Learning models by harnessing GPU compute. PyTorch is extensively used for rapid prototyping in research and small scale projects. The objective of this article is to give you a hands on experience with PyTorch & some basic mathematical lingo associated with Deep Learning. We also introduce the classic problem of [Handwritten Digit Recognition](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "**Table of Contents**:\n",
    "\n",
    "- [PyTorch Introduction](#pytorch-introduction)\n",
    "- [Tensors](#tensor)\n",
    "- [Variables & Autograd](#variables-and-autograd)\n",
    "- [Logistic Regression](#logistic-regression)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Introduction\n",
    "\n",
    "PyTorch is a Python based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A Deep Learning research platform that provides maximum flexibility and speed through Dynamic Compute graphs & Imperative Programming control flow.\n",
    "- A replacement for NumPy to harness GPU compute capability.\n",
    "\n",
    "Here's a list of modules we'll need in order to run this tutorial:\n",
    "\n",
    "1. [torch.autograd](http://pytorch.org/docs/master/autograd.html) Provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\n",
    "2. [torch.nn](http://pytorch.org/docs/master/nn.html) Package provides an easy and modular way to build and train simple or complex neural networks.\n",
    "3. [NumPy](http://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "\n",
    "#### Running Code Cell\n",
    "\n",
    "If you want to run Cell code using shortcut, type **`shift + enter`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the package we need to run the tutorial\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Is CUDA available on this instance?\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Seed for reproducibility\n",
    "# This make sure you get the same results returned in this notebook\n",
    "torch.manual_seed(1)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "In any deep learning pipeline, one obvious inevitable thing that we encounter, is mathematical data. Be it an images stored in the form of `[height x width]` matrices, a piece of text stored in the form a vector or some spooky operation taking place between those two. PyTorch provides us with objects known as Tensors that store all this data under one roof.\n",
    "\n",
    "*Formally*, a [PyTorch Tensor](http://pytorch.org/docs/master/tensors.html) is conceptually identical to a NumPy's `ndarray`, and PyTorch provides many functions for operating on these Tensors. Like standard `ndarrays`, PyTorch Tensors do not know anything about deep learning or computational graphs or gradients; they are a generic tool for scientific computing. We can use n-dimensional Tensors to our requirement, for instance - we can have multidimensional (2D) tensor storing an image, or a single variable storing text.\n",
    "\n",
    "The following snippets demonstrate Tensors & a few of their operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Tensor(5, 3):\n",
      "\n",
      "1.00000e-16 *\n",
      "  8.8183  0.0000  8.8183\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Tensor on GPU if CUDA available\n",
    "dtype = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# Construct a 5x3 matrix, uninitialized:\n",
    "print(\"torch.Tensor(5, 3):\")\n",
    "x = torch.Tensor(5, 3).type(dtype)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.rand(5, 3):\n",
      "\n",
      " 0.1863  0.3879  0.3456\n",
      " 0.6697  0.3968  0.9355\n",
      " 0.5388  0.8463  0.4192\n",
      " 0.3133  0.6852  0.5245\n",
      " 0.2045  0.4435  0.8781\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct a randomly initialized matrix\n",
    "print(\"torch.rand(5, 3):\")\n",
    "x = torch.rand(5, 3).type(dtype)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Tensor Size:\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# Get its size\n",
    "print(\"Last Tensor Size:\")\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax 1: x + y =\n",
      "\n",
      " 0.4158  0.4153  0.8800\n",
      " 1.3402  1.3107  1.3528\n",
      " 0.9960  1.4050  0.8499\n",
      " 0.4537  1.6243  0.7226\n",
      " 0.9828  1.2442  1.5941\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are multiple syntaxes for operations. Let’s see addition as an example\n",
    "# Addition: syntax 1\n",
    "y = torch.rand(5, 3)\n",
    "print(\"Syntax 1: x + y =\")\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax 2: torch.add(x, y) =\n",
      "\n",
      " 0.4158  0.4153  0.8800\n",
      " 1.3402  1.3107  1.3528\n",
      " 0.9960  1.4050  0.8499\n",
      " 0.4537  1.6243  0.7226\n",
      " 0.9828  1.2442  1.5941\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition: syntax 2\n",
    "print(\"Syntax 2: torch.add(x, y) =\")\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax 3: torch.add(x, y, out=result) =\n",
      "\n",
      " 0.4158  0.4153  0.8800\n",
      " 1.3402  1.3107  1.3528\n",
      " 0.9960  1.4050  0.8499\n",
      " 0.4537  1.6243  0.7226\n",
      " 0.9828  1.2442  1.5941\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition: giving an output tensor\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(\"Syntax 3: torch.add(x, y, out=result) =\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-place Addition: y.add_(x) =\n",
      "\n",
      " 0.4158  0.4153  0.8800\n",
      " 1.3402  1.3107  1.3528\n",
      " 0.9960  1.4050  0.8499\n",
      " 0.4537  1.6243  0.7226\n",
      " 0.9828  1.2442  1.5941\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition: in-place\n",
    "# adds x to y\n",
    "print(\"In-place Addition: y.add_(x) =\")\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing x[:, 1] - Second column(index starts from zero) of every rows:\n",
      "\n",
      " 0.3879\n",
      " 0.3968\n",
      " 0.8463\n",
      " 0.6852\n",
      " 0.4435\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can use standard numpy-like indexing with all bells and whistles!\n",
    "print (\"Indexing x[:, 1] - Second column(index starts from zero) of every rows:\")\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NumPy `ndarrays`, PyTorch Tensors can utilize GPUs to accelerate their numeric computations & PyTorch makes it ridiculously easy to switch from GPU to CPU & vice versa.\n",
    "\n",
    "*Note: It is interesting to know that PyTorch can serve as a full fledged replacement for NumPy, as Tensors & ndarrays can be used interchangeably.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy tensor:  [[-1.11230897 -0.04769943 -0.8689276   0.11765421]\n",
      " [-2.26887639  0.37423547  0.64090709 -1.03655458]\n",
      " [-0.3089939   0.95988481 -0.67968412  0.30984516]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a sample matrix of 3 rows and 4 columns \n",
    "# from a Normal Distribution with Mean 0 and Var 1 \n",
    "numpy_tensor = np.random.randn(3, 4)\n",
    "print (\"Numpy tensor: \", numpy_tensor, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy to PyTorch tensor:  \n",
      "-1.1123 -0.0477 -0.8689  0.1177\n",
      "-2.2689  0.3742  0.6409 -1.0366\n",
      "-0.3090  0.9599 -0.6797  0.3098\n",
      "[torch.FloatTensor of size 3x4]\n",
      " \n",
      "\n",
      "PyTorch to Numpy tensor:  [[-1.11230897 -0.04769943 -0.8689276   0.11765421]\n",
      " [-2.26887639  0.37423547  0.64090709 -1.03655458]\n",
      " [-0.3089939   0.95988481 -0.67968412  0.30984516]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy array to pytorch array\n",
    "pytorch_tensor = torch.Tensor(numpy_tensor)\n",
    "print (\"Numpy to PyTorch tensor: \", pytorch_tensor, \"\\n\")\n",
    "# Or another way\n",
    "pytorch_tensor = torch.from_numpy(numpy_tensor)\n",
    "# Convert torch tensor to numpy representation\n",
    "print (\"PyTorch to Numpy tensor: \", pytorch_tensor.numpy(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: you can run the GPU to CPU example only if you are running a FloydHub GPU instance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If cuda is available, run GPU-to-CPU and vice versa example\n",
    "if cuda:\n",
    "    # If we want to use tensor on GPU provide another type\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    gpu_tensor = torch.randn(5, 10).type(dtype)\n",
    "    # Or just call `cuda()` method\n",
    "    gpu_tensor = pytorch_tensor.cuda()\n",
    "    print (\"PyTorch cuda gpu_tensor \", gpu_tensor, \"\\n\")\n",
    "    # Call back to the CPU\n",
    "    cpu_tensor = gpu_tensor.cpu()\n",
    "    print (\"PyTorch cuda tensor to cpu_tensor, gpu_tensor.cpu() \", cpu_tensor, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define pytorch tensors\n",
    "x = torch.randn(10, 20)\n",
    "y = torch.ones(20, 5)\n",
    "# `@` mean matrix multiplication from python3.5, PEP-0465\n",
    "res = x @ y # Same as torch.matmul(x, y)\n",
    "\n",
    "# Get the shape\n",
    "res.shape  # torch.Size([10, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Exercise\n",
    "\n",
    "Create a square Matrix as this: [[1, 2], [3, 4]], multiply it for this *column* vector [5, 6] and add at the result the following column vector [7, 8]. The result should be a column vector with these values: [24, 47].\n",
    "\n",
    "`result = matrix * column1 + column2`\n",
    "\n",
    "*Note: There is more than one way to achieve the same result.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 24\n",
      " 47\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a 2-D Tensor with these values [[1, 2], [3, 4]]\n",
    "matrix = # CODE HERE\n",
    "\n",
    "# Create a column Vector with these values [5, 6]\n",
    "column1 = # CODE HERE\n",
    "\n",
    "# Create a column Vector with these values [7, 8]\n",
    "column2 = # CODE HERE\n",
    "\n",
    "# Matrix Mult\n",
    "mutl = # CODE HERE\n",
    "\n",
    "# Addition\n",
    "res = # CODE HERE\n",
    "\n",
    "# Show the result\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and AutoGrad\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://pytorch.org/tutorials/_images/Variable.png\"/>\n",
    "</p>\n",
    "\n",
    ">Credits: PyTorch Variable Docs\n",
    "\n",
    "Variables are **wrappers** over Tensors that allow them to be differentiated & modified. Let me demonstrate how: Take the example in the following snippet, where we apply a string of operations over a 'Variable' `x`, to predict `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "x + 2 = y, Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      " \n",
      "\n",
      "y was created as a result of an operation, so we have  <torch.autograd.function.AddConstantBackward object at 0x7f4a140a2e58> \n",
      "\n",
      "y * y * * 3 = z Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " \n",
      " mean(z),  Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "After backprop, x Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Var and Authograd example on simple operations\n",
    "\n",
    "# Create a Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(\"x\", x)\n",
    "\n",
    "# Make an op\n",
    "y = x + 2\n",
    "print(\"x + 2 = y,\", y, \"\\n\")\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn.\n",
    "print(\"y was created as a result of an operation, so we have \", y.grad_fn, \"\\n\")\n",
    "\n",
    "# More op\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(\"y * y * * 3 = z\", z, \"\\n\", \"mean(z), \", out)\n",
    "\n",
    "# Let’s backprop now \n",
    "out.backward()\n",
    "print(\"After backprop, x\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 10, 10, 10, 1\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "\n",
    "### A typical forward computation\n",
    "# Matrix Mult\n",
    "h = x.mm(w1)\n",
    "# Custom ReLU\n",
    "h_relu = h.clamp(min=0)\n",
    "# Matrix Mult\n",
    "y_pred = h_relu.mm(w2)\n",
    "\n",
    "# Compute the loss with Mean Squared Error\n",
    "loss = ((y_pred - y).pow(2).sum())/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we wish to compute the derivative of this function with respect to the loss. Using the traditional symbolic differentiation, we would achieve that in a way like this,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "grad_y_pred = 2.0 * (y_pred - y)\n",
    "grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "grad_h = grad_h_relu.clone()\n",
    "grad_h[h < 0] = 0\n",
    "grad_w1 = x.t().mm(grad_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This process mentioned up is a part of Backpropagation in a simple single layer Neural Network, don't worry about it even if you don't understand much of it, we'll cover it in the next article.\n",
    "\n",
    "Now imagine, if there were tens of different types of mathematical operations before computing `loss` in the first snippet (because there will be in what's about to come!). How could you possibly code the gradient computation for something like that? Thankfully, `torch.autograd` exists. It works on the principle of Automatic Differentiation, which is inherently based on the **chain rule**. To perform the gradient computation in the above example using `autograd`, all we have to do is,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "#Make sure x, w1 & w2 are Variables\n",
    "y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "# Compute the loss with Mean Squared Error\n",
    "loss = ((y_pred - y).pow(2).sum())/N\n",
    "\n",
    "# Auto differentation\n",
    "loss.backward()\n",
    "\n",
    "# Get Grads with <variable_name>.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every variable instance has two attributes: `.data` that contains the initial tensor itself and `.grad` that contains gradients for the corresponding tensor. Here are some more snippets on using Autograd & Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "# Do some operation\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "# Let’s compute the gradient now\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: When we wrap our Tensors with Variables, the arithmetic operations still remain the same, but Variables also remember their history of computation. Thus, `z` is not only a regular `2 x 2` Tensor but expression, involving `y` & `x`. This is what helps us define a **computational graph**. Nodes in this graph are Tensors & edges will be Functions operated on these nodes. **Backpropagating** through this graph then allows you to easily compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables and AutoGrad Exercise\n",
    "\n",
    "In the previous snippet of code we have used auto differentiation to compute the gradient with respect to the loss. Now we will use the computed gradients to update the parameters simulating an Optimization workflow with [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).\n",
    "\n",
    "*Note: You should see the loss fastly converging to zero, if is not the case, you have to fix your code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471615.84375\n",
      "471615.84375\n",
      "1265843.5\n",
      "1265843.5\n",
      "744764.25\n",
      "744764.25\n",
      "1196102.0\n",
      "1196102.0\n",
      "852086.5625\n",
      "852086.5625\n",
      "117779.265625\n",
      "117779.265625\n",
      "9248.3505859375\n",
      "9248.3505859375\n",
      "7311.44189453125\n",
      "7311.44189453125\n",
      "6044.49267578125\n",
      "6044.49267578125\n",
      "5061.51123046875\n",
      "5061.51123046875\n",
      "4283.59814453125\n",
      "4283.59814453125\n",
      "3659.133056640625\n",
      "3659.133056640625\n",
      "3151.992919921875\n",
      "3151.992919921875\n",
      "2734.068603515625\n",
      "2734.068603515625\n",
      "2386.427734375\n",
      "2386.427734375\n",
      "2094.23193359375\n",
      "2094.23193359375\n",
      "1846.6749267578125\n",
      "1846.6749267578125\n",
      "1635.353759765625\n",
      "1635.353759765625\n",
      "1453.8814697265625\n",
      "1453.8814697265625\n",
      "1297.270263671875\n",
      "1297.270263671875\n",
      "1161.3912353515625\n",
      "1161.3912353515625\n",
      "1042.9434814453125\n",
      "1042.9434814453125\n",
      "939.2108764648438\n",
      "939.2108764648438\n",
      "848.0303955078125\n",
      "848.0303955078125\n",
      "767.5533447265625\n",
      "767.5533447265625\n",
      "696.3588256835938\n",
      "696.3588256835938\n",
      "633.1547241210938\n",
      "633.1547241210938\n",
      "576.8418579101562\n",
      "576.8418579101562\n",
      "526.5444946289062\n",
      "526.5444946289062\n",
      "481.5548400878906\n",
      "481.5548400878906\n",
      "441.1895751953125\n",
      "441.1895751953125\n",
      "404.888427734375\n",
      "404.888427734375\n",
      "372.15863037109375\n",
      "372.15863037109375\n",
      "342.5877380371094\n",
      "342.5877380371094\n",
      "315.8384094238281\n",
      "315.8384094238281\n",
      "291.5721130371094\n",
      "291.5721130371094\n",
      "269.5362854003906\n",
      "269.5362854003906\n",
      "249.48577880859375\n",
      "249.48577880859375\n",
      "231.23104858398438\n",
      "231.23104858398438\n",
      "214.597412109375\n",
      "214.597412109375\n",
      "199.40045166015625\n",
      "199.40045166015625\n",
      "185.48960876464844\n",
      "185.48960876464844\n",
      "172.73202514648438\n",
      "172.73202514648438\n",
      "161.03878784179688\n",
      "161.03878784179688\n",
      "150.30337524414062\n",
      "150.30337524414062\n",
      "140.4242401123047\n",
      "140.4242401123047\n",
      "131.31971740722656\n",
      "131.31971740722656\n",
      "122.91970825195312\n",
      "122.91970825195312\n",
      "115.16171264648438\n",
      "115.16171264648438\n",
      "107.98260498046875\n",
      "107.98260498046875\n",
      "101.3367919921875\n",
      "101.3367919921875\n",
      "95.1820297241211\n",
      "95.1820297241211\n",
      "89.47720336914062\n",
      "89.47720336914062\n",
      "84.17998504638672\n",
      "84.17998504638672\n",
      "79.25565338134766\n",
      "79.25565338134766\n",
      "74.67547607421875\n",
      "74.67547607421875\n",
      "70.40926361083984\n",
      "70.40926361083984\n",
      "66.43291473388672\n",
      "66.43291473388672\n",
      "62.724552154541016\n",
      "62.724552154541016\n",
      "59.26249694824219\n",
      "59.26249694824219\n",
      "56.02762985229492\n",
      "56.02762985229492\n",
      "53.002662658691406\n",
      "53.002662658691406\n",
      "50.17259979248047\n",
      "50.17259979248047\n",
      "47.52269744873047\n",
      "47.52269744873047\n",
      "45.039642333984375\n",
      "45.039642333984375\n",
      "42.713069915771484\n",
      "42.713069915771484\n",
      "40.53005599975586\n",
      "40.53005599975586\n",
      "38.48044967651367\n",
      "38.48044967651367\n",
      "36.55393981933594\n",
      "36.55393981933594\n",
      "34.742427825927734\n",
      "34.742427825927734\n",
      "33.03785705566406\n",
      "33.03785705566406\n",
      "31.432899475097656\n",
      "31.432899475097656\n",
      "29.92091941833496\n",
      "29.92091941833496\n",
      "28.495319366455078\n",
      "28.495319366455078\n",
      "27.151588439941406\n",
      "27.151588439941406\n",
      "25.883821487426758\n",
      "25.883821487426758\n",
      "24.686689376831055\n",
      "24.686689376831055\n",
      "23.555593490600586\n",
      "23.555593490600586\n",
      "22.486186981201172\n",
      "22.486186981201172\n",
      "21.47441864013672\n",
      "21.47441864013672\n",
      "20.516817092895508\n",
      "20.516817092895508\n",
      "19.61013412475586\n",
      "19.61013412475586\n",
      "18.751134872436523\n",
      "18.751134872436523\n",
      "17.936992645263672\n",
      "17.936992645263672\n",
      "17.164915084838867\n",
      "17.164915084838867\n",
      "16.432559967041016\n",
      "16.432559967041016\n",
      "15.737245559692383\n",
      "15.737245559692383\n",
      "15.0768461227417\n",
      "15.0768461227417\n",
      "14.449506759643555\n",
      "14.449506759643555\n",
      "13.853433609008789\n",
      "13.853433609008789\n",
      "13.28664779663086\n",
      "13.28664779663086\n",
      "12.747360229492188\n",
      "12.747360229492188\n",
      "12.234305381774902\n",
      "12.234305381774902\n",
      "11.74550724029541\n",
      "11.74550724029541\n",
      "11.279787063598633\n",
      "11.279787063598633\n",
      "10.836088180541992\n",
      "10.836088180541992\n",
      "10.412970542907715\n",
      "10.412970542907715\n",
      "10.00947093963623\n",
      "10.00947093963623\n",
      "9.624497413635254\n",
      "9.624497413635254\n",
      "9.256962776184082\n",
      "9.256962776184082\n",
      "8.90601634979248\n",
      "8.90601634979248\n",
      "8.570712089538574\n",
      "8.570712089538574\n",
      "8.250476837158203\n",
      "8.250476837158203\n",
      "7.944380760192871\n",
      "7.944380760192871\n",
      "7.651645183563232\n",
      "7.651645183563232\n",
      "7.371646404266357\n",
      "7.371646404266357\n",
      "7.103634834289551\n",
      "7.103634834289551\n",
      "6.8470563888549805\n",
      "6.8470563888549805\n",
      "6.6013875007629395\n",
      "6.6013875007629395\n",
      "6.366026401519775\n",
      "6.366026401519775\n",
      "6.140529155731201\n",
      "6.140529155731201\n",
      "5.924401760101318\n",
      "5.924401760101318\n",
      "5.717132568359375\n",
      "5.717132568359375\n",
      "5.518469333648682\n",
      "5.518469333648682\n",
      "5.3277482986450195\n",
      "5.3277482986450195\n",
      "5.144662380218506\n",
      "5.144662380218506\n",
      "4.968921184539795\n",
      "4.968921184539795\n",
      "4.800142765045166\n",
      "4.800142765045166\n",
      "4.638034343719482\n",
      "4.638034343719482\n",
      "4.482282638549805\n",
      "4.482282638549805\n",
      "4.332596302032471\n",
      "4.332596302032471\n",
      "4.188666343688965\n",
      "4.188666343688965\n",
      "4.050277233123779\n",
      "4.050277233123779\n",
      "3.9171648025512695\n",
      "3.9171648025512695\n",
      "3.789086103439331\n",
      "3.789086103439331\n",
      "3.6658363342285156\n",
      "3.6658363342285156\n",
      "3.5472207069396973\n",
      "3.5472207069396973\n",
      "3.43300724029541\n",
      "3.43300724029541\n",
      "3.3230397701263428\n",
      "3.3230397701263428\n",
      "3.2170512676239014\n",
      "3.2170512676239014\n",
      "3.114971876144409\n",
      "3.114971876144409\n",
      "3.0166208744049072\n",
      "3.0166208744049072\n",
      "2.9218454360961914\n",
      "2.9218454360961914\n",
      "2.8304097652435303\n",
      "2.8304097652435303\n",
      "2.7422304153442383\n",
      "2.7422304153442383\n",
      "2.6571550369262695\n",
      "2.6571550369262695\n",
      "2.575103998184204\n",
      "2.575103998184204\n",
      "2.4958934783935547\n",
      "2.4958934783935547\n",
      "2.419440269470215\n",
      "2.419440269470215\n",
      "2.3456554412841797\n",
      "2.3456554412841797\n",
      "2.274409770965576\n",
      "2.274409770965576\n",
      "2.2055835723876953\n",
      "2.2055835723876953\n",
      "2.139113664627075\n",
      "2.139113664627075\n",
      "2.074910879135132\n",
      "2.074910879135132\n",
      "2.0128555297851562\n",
      "2.0128555297851562\n",
      "1.9528692960739136\n",
      "1.9528692960739136\n",
      "1.8949233293533325\n",
      "1.8949233293533325\n",
      "1.8388599157333374\n",
      "1.8388599157333374\n",
      "1.7847076654434204\n",
      "1.7847076654434204\n",
      "1.7323358058929443\n",
      "1.7323358058929443\n",
      "1.681635856628418\n",
      "1.681635856628418\n",
      "1.6325745582580566\n",
      "1.6325745582580566\n",
      "1.5851389169692993\n",
      "1.5851389169692993\n",
      "1.5391948223114014\n",
      "1.5391948223114014\n",
      "1.4947352409362793\n",
      "1.4947352409362793\n",
      "1.4517027139663696\n",
      "1.4517027139663696\n",
      "1.4100446701049805\n",
      "1.4100446701049805\n",
      "1.3697071075439453\n",
      "1.3697071075439453\n",
      "1.330635666847229\n",
      "1.330635666847229\n",
      "1.2927982807159424\n",
      "1.2927982807159424\n",
      "1.2561378479003906\n",
      "1.2561378479003906\n",
      "1.220615267753601\n",
      "1.220615267753601\n",
      "1.1862050294876099\n",
      "1.1862050294876099\n",
      "1.1528433561325073\n",
      "1.1528433561325073\n",
      "1.1205672025680542\n",
      "1.1205672025680542\n",
      "1.089228630065918\n",
      "1.089228630065918\n",
      "1.0588456392288208\n",
      "1.0588456392288208\n",
      "1.0293792486190796\n",
      "1.0293792486190796\n",
      "1.0008140802383423\n",
      "1.0008140802383423\n",
      "0.97310471534729\n",
      "0.97310471534729\n",
      "0.9462330341339111\n",
      "0.9462330341339111\n",
      "0.920160710811615\n",
      "0.920160710811615\n",
      "0.8948603272438049\n",
      "0.8948603272438049\n",
      "0.8703317046165466\n",
      "0.8703317046165466\n",
      "0.8465156555175781\n",
      "0.8465156555175781\n",
      "0.8234080076217651\n",
      "0.8234080076217651\n",
      "0.8009749054908752\n",
      "0.8009749054908752\n",
      "0.7791981101036072\n",
      "0.7791981101036072\n",
      "0.7580875158309937\n",
      "0.7580875158309937\n",
      "0.7375842332839966\n",
      "0.7375842332839966\n",
      "0.717655599117279\n",
      "0.717655599117279\n",
      "0.6983128786087036\n",
      "0.6983128786087036\n",
      "0.6795164346694946\n",
      "0.6795164346694946\n",
      "0.6612633466720581\n",
      "0.6612633466720581\n",
      "0.6435434222221375\n",
      "0.6435434222221375\n",
      "0.6263333559036255\n",
      "0.6263333559036255\n",
      "0.6096019148826599\n",
      "0.6096019148826599\n",
      "0.5933545231819153\n",
      "0.5933545231819153\n",
      "0.5775662660598755\n",
      "0.5775662660598755\n",
      "0.562222421169281\n",
      "0.562222421169281\n",
      "0.5473113656044006\n",
      "0.5473113656044006\n",
      "0.532823920249939\n",
      "0.532823920249939\n",
      "0.5187601447105408\n",
      "0.5187601447105408\n",
      "0.5050762891769409\n",
      "0.5050762891769409\n",
      "0.4917723536491394\n",
      "0.4917723536491394\n",
      "0.4788418710231781\n",
      "0.4788418710231781\n",
      "0.4662574827671051\n",
      "0.4662574827671051\n",
      "0.45403382182121277\n",
      "0.45403382182121277\n",
      "0.4421400725841522\n",
      "0.4421400725841522\n",
      "0.4305805563926697\n",
      "0.4305805563926697\n",
      "0.4193398058414459\n",
      "0.4193398058414459\n",
      "0.4084022343158722\n",
      "0.4084022343158722\n",
      "0.39776965975761414\n",
      "0.39776965975761414\n",
      "0.3874269723892212\n",
      "0.3874269723892212\n",
      "0.37736618518829346\n",
      "0.37736618518829346\n",
      "0.3675917387008667\n",
      "0.3675917387008667\n",
      "0.3580775856971741\n",
      "0.3580775856971741\n",
      "0.3488157391548157\n",
      "0.3488157391548157\n",
      "0.3398033082485199\n",
      "0.3398033082485199\n",
      "0.33102908730506897\n",
      "0.33102908730506897\n",
      "0.3224918842315674\n",
      "0.3224918842315674\n",
      "0.3141893446445465\n",
      "0.3141893446445465\n",
      "0.3061079978942871\n",
      "0.3061079978942871\n",
      "0.2982423007488251\n",
      "0.2982423007488251\n",
      "0.2905854880809784\n",
      "0.2905854880809784\n",
      "0.2831352949142456\n",
      "0.2831352949142456\n",
      "0.2758849263191223\n",
      "0.2758849263191223\n",
      "0.26883482933044434\n",
      "0.26883482933044434\n",
      "0.2619684040546417\n",
      "0.2619684040546417\n",
      "0.25528183579444885\n",
      "0.25528183579444885\n",
      "0.2487725168466568\n",
      "0.2487725168466568\n",
      "0.24243582785129547\n",
      "0.24243582785129547\n",
      "0.2362620234489441\n",
      "0.2362620234489441\n",
      "0.23024895787239075\n",
      "0.23024895787239075\n",
      "0.2244068682193756\n",
      "0.2244068682193756\n",
      "0.2187095582485199\n",
      "0.2187095582485199\n",
      "0.21316403150558472\n",
      "0.21316403150558472\n",
      "0.2077641487121582\n",
      "0.2077641487121582\n",
      "0.20249949395656586\n",
      "0.20249949395656586\n",
      "0.19738702476024628\n",
      "0.19738702476024628\n",
      "0.19240106642246246\n",
      "0.19240106642246246\n",
      "0.1875450611114502\n",
      "0.1875450611114502\n",
      "0.1828145831823349\n",
      "0.1828145831823349\n",
      "0.17820508778095245\n",
      "0.17820508778095245\n",
      "0.17371639609336853\n",
      "0.17371639609336853\n",
      "0.16934461891651154\n",
      "0.16934461891651154\n",
      "0.16508543491363525\n",
      "0.16508543491363525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16093353927135468\n",
      "0.16093353927135468\n",
      "0.1568954586982727\n",
      "0.1568954586982727\n",
      "0.1529628038406372\n",
      "0.1529628038406372\n",
      "0.14912675321102142\n",
      "0.14912675321102142\n",
      "0.14539843797683716\n",
      "0.14539843797683716\n",
      "0.14176201820373535\n",
      "0.14176201820373535\n",
      "0.13821689784526825\n",
      "0.13821689784526825\n",
      "0.1347638964653015\n",
      "0.1347638964653015\n",
      "0.1313978135585785\n",
      "0.1313978135585785\n",
      "0.12811818718910217\n",
      "0.12811818718910217\n",
      "0.12492497265338898\n",
      "0.12492497265338898\n",
      "0.12180949002504349\n",
      "0.12180949002504349\n",
      "0.11877629160881042\n",
      "0.11877629160881042\n",
      "0.115820974111557\n",
      "0.115820974111557\n",
      "0.11293844878673553\n",
      "0.11293844878673553\n",
      "0.11013401299715042\n",
      "0.11013401299715042\n",
      "0.10739827901124954\n",
      "0.10739827901124954\n",
      "0.10473249852657318\n",
      "0.10473249852657318\n",
      "0.10213278234004974\n",
      "0.10213278234004974\n",
      "0.09959892928600311\n",
      "0.09959892928600311\n",
      "0.09712858498096466\n",
      "0.09712858498096466\n",
      "0.09472343325614929\n",
      "0.09472343325614929\n",
      "0.09237639605998993\n",
      "0.09237639605998993\n",
      "0.09009081870317459\n",
      "0.09009081870317459\n",
      "0.08786336332559586\n",
      "0.08786336332559586\n",
      "0.08568955957889557\n",
      "0.08568955957889557\n",
      "0.08357613533735275\n",
      "0.08357613533735275\n",
      "0.08151395618915558\n",
      "0.08151395618915558\n",
      "0.07950051128864288\n",
      "0.07950051128864288\n",
      "0.07753698527812958\n",
      "0.07753698527812958\n",
      "0.07562565058469772\n",
      "0.07562565058469772\n",
      "0.07376040518283844\n",
      "0.07376040518283844\n",
      "0.07194197922945023\n",
      "0.07194197922945023\n",
      "0.07017092406749725\n",
      "0.07017092406749725\n",
      "0.06844311207532883\n",
      "0.06844311207532883\n",
      "0.06675822287797928\n",
      "0.06675822287797928\n",
      "0.06511875241994858\n",
      "0.06511875241994858\n",
      "0.06351920962333679\n",
      "0.06351920962333679\n",
      "0.06195672228932381\n",
      "0.06195672228932381\n",
      "0.06043577194213867\n",
      "0.06043577194213867\n",
      "0.058952514082193375\n",
      "0.058952514082193375\n",
      "0.05750475823879242\n",
      "0.05750475823879242\n",
      "0.056092459708452225\n",
      "0.056092459708452225\n",
      "0.05471644178032875\n",
      "0.05471644178032875\n",
      "0.05337679758667946\n",
      "0.05337679758667946\n",
      "0.05206775665283203\n",
      "0.05206775665283203\n",
      "0.05079208314418793\n",
      "0.05079208314418793\n",
      "0.04954998940229416\n",
      "0.04954998940229416\n",
      "0.048338036984205246\n",
      "0.048338036984205246\n",
      "0.04715351015329361\n",
      "0.04715351015329361\n",
      "0.046000637114048004\n",
      "0.046000637114048004\n",
      "0.044875212013721466\n",
      "0.044875212013721466\n",
      "0.043778590857982635\n",
      "0.043778590857982635\n",
      "0.04270794615149498\n",
      "0.04270794615149498\n",
      "0.0416640043258667\n",
      "0.0416640043258667\n",
      "0.04064755514264107\n",
      "0.04064755514264107\n",
      "0.03965526074171066\n",
      "0.03965526074171066\n",
      "0.03868937864899635\n",
      "0.03868937864899635\n",
      "0.03774555027484894\n",
      "0.03774555027484894\n",
      "0.036825571209192276\n",
      "0.036825571209192276\n",
      "0.03592725843191147\n",
      "0.03592725843191147\n",
      "0.035050857812166214\n",
      "0.035050857812166214\n",
      "0.03419683128595352\n",
      "0.03419683128595352\n",
      "0.03336368873715401\n",
      "0.03336368873715401\n",
      "0.032552603632211685\n",
      "0.032552603632211685\n",
      "0.03175861015915871\n",
      "0.03175861015915871\n",
      "0.030985040590167046\n",
      "0.030985040590167046\n",
      "0.030233802273869514\n",
      "0.030233802273869514\n",
      "0.029496939852833748\n",
      "0.029496939852833748\n",
      "0.028780490159988403\n",
      "0.028780490159988403\n",
      "0.028079861775040627\n",
      "0.028079861775040627\n",
      "0.027396604418754578\n",
      "0.027396604418754578\n",
      "0.026731053367257118\n",
      "0.026731053367257118\n",
      "0.026080859825015068\n",
      "0.026080859825015068\n",
      "0.02544730342924595\n",
      "0.02544730342924595\n",
      "0.024829600006341934\n",
      "0.024829600006341934\n",
      "0.024228980764746666\n",
      "0.024228980764746666\n",
      "0.02364160306751728\n",
      "0.02364160306751728\n",
      "0.023067142814397812\n",
      "0.023067142814397812\n",
      "0.02250783145427704\n",
      "0.02250783145427704\n",
      "0.021961437538266182\n",
      "0.021961437538266182\n",
      "0.021430019289255142\n",
      "0.021430019289255142\n",
      "0.020910415798425674\n",
      "0.020910415798425674\n",
      "0.020402908325195312\n",
      "0.020402908325195312\n",
      "0.019908586516976357\n",
      "0.019908586516976357\n",
      "0.01942758448421955\n",
      "0.01942758448421955\n",
      "0.01895706169307232\n",
      "0.01895706169307232\n",
      "0.018498197197914124\n",
      "0.018498197197914124\n",
      "0.018049856647849083\n",
      "0.018049856647849083\n",
      "0.017613263800740242\n",
      "0.017613263800740242\n",
      "0.0171870868653059\n",
      "0.0171870868653059\n",
      "0.016771554946899414\n",
      "0.016771554946899414\n",
      "0.016366582363843918\n",
      "0.016366582363843918\n",
      "0.015970628708600998\n",
      "0.015970628708600998\n",
      "0.015584588050842285\n",
      "0.015584588050842285\n",
      "0.015207714401185513\n",
      "0.015207714401185513\n",
      "0.01484076026827097\n",
      "0.01484076026827097\n",
      "0.014482197351753712\n",
      "0.014482197351753712\n",
      "0.014131693169474602\n",
      "0.014131693169474602\n",
      "0.013790791854262352\n",
      "0.013790791854262352\n",
      "0.013458197936415672\n",
      "0.013458197936415672\n",
      "0.01313424576073885\n",
      "0.01313424576073885\n",
      "0.01281591784209013\n",
      "0.01281591784209013\n",
      "0.012506579980254173\n",
      "0.012506579980254173\n",
      "0.012205311097204685\n",
      "0.012205311097204685\n",
      "0.011911054141819477\n",
      "0.011911054141819477\n",
      "0.011623871512711048\n",
      "0.011623871512711048\n",
      "0.011343340389430523\n",
      "0.011343340389430523\n",
      "0.01106930710375309\n",
      "0.01106930710375309\n",
      "0.010802960023283958\n",
      "0.010802960023283958\n",
      "0.010543261654675007\n",
      "0.010543261654675007\n",
      "0.010288150049746037\n",
      "0.010288150049746037\n",
      "0.010040797293186188\n",
      "0.010040797293186188\n",
      "0.009798433631658554\n",
      "0.009798433631658554\n",
      "0.00956298690289259\n",
      "0.00956298690289259\n",
      "0.00933267455548048\n",
      "0.00933267455548048\n",
      "0.00910835713148117\n",
      "0.00910835713148117\n",
      "0.008888961747288704\n",
      "0.008888961747288704\n",
      "0.008674973621964455\n",
      "0.008674973621964455\n",
      "0.008465982042253017\n",
      "0.008465982042253017\n",
      "0.00826268456876278\n",
      "0.00826268456876278\n",
      "0.008063710294663906\n",
      "0.008063710294663906\n",
      "0.007869758643209934\n",
      "0.007869758643209934\n",
      "0.007680001202970743\n",
      "0.007680001202970743\n",
      "0.007495856378227472\n",
      "0.007495856378227472\n",
      "0.007314879447221756\n",
      "0.007314879447221756\n",
      "0.007139378227293491\n",
      "0.007139378227293491\n",
      "0.006967440713196993\n",
      "0.006967440713196993\n",
      "0.0067997588776052\n",
      "0.0067997588776052\n",
      "0.006636453792452812\n",
      "0.006636453792452812\n",
      "0.006477678660303354\n",
      "0.006477678660303354\n",
      "0.0063219815492630005\n",
      "0.0063219815492630005\n",
      "0.006170201580971479\n",
      "0.006170201580971479\n",
      "0.0060218521393835545\n",
      "0.0060218521393835545\n",
      "0.005877197254449129\n",
      "0.005877197254449129\n",
      "0.005736299324780703\n",
      "0.005736299324780703\n",
      "0.005598205141723156\n",
      "0.005598205141723156\n",
      "0.005463793873786926\n",
      "0.005463793873786926\n",
      "0.005333178211003542\n",
      "0.005333178211003542\n",
      "0.005205185152590275\n",
      "0.005205185152590275\n",
      "0.005080294329673052\n",
      "0.005080294329673052\n",
      "0.004958400968462229\n",
      "0.004958400968462229\n",
      "0.004839156288653612\n",
      "0.004839156288653612\n",
      "0.004723156336694956\n",
      "0.004723156336694956\n",
      "0.004610129166394472\n",
      "0.004610129166394472\n",
      "0.004499209113419056\n",
      "0.004499209113419056\n",
      "0.004391578957438469\n",
      "0.004391578957438469\n",
      "0.004286367911845446\n",
      "0.004286367911845446\n",
      "0.004183195065706968\n",
      "0.004183195065706968\n",
      "0.004083376377820969\n",
      "0.004083376377820969\n",
      "0.003985540941357613\n",
      "0.003985540941357613\n",
      "0.0038900405634194613\n",
      "0.0038900405634194613\n",
      "0.003796642180532217\n",
      "0.003796642180532217\n",
      "0.0037057155277580023\n",
      "0.0037057155277580023\n",
      "0.003617530455812812\n",
      "0.003617530455812812\n",
      "0.003530424553900957\n",
      "0.003530424553900957\n",
      "0.0034459340386092663\n",
      "0.0034459340386092663\n",
      "0.0033632705453783274\n",
      "0.0033632705453783274\n",
      "0.0032826801761984825\n",
      "0.0032826801761984825\n",
      "0.0032040460500866175\n",
      "0.0032040460500866175\n",
      "0.003127661533653736\n",
      "0.003127661533653736\n",
      "0.0030525201000273228\n",
      "0.0030525201000273228\n",
      "0.0029793514404445887\n",
      "0.0029793514404445887\n",
      "0.002908379305154085\n",
      "0.002908379305154085\n",
      "0.0028387680649757385\n",
      "0.0028387680649757385\n",
      "0.002770881401374936\n",
      "0.002770881401374936\n",
      "0.00270490231923759\n",
      "0.00270490231923759\n",
      "0.002640013350173831\n",
      "0.002640013350173831\n",
      "0.0025767663028091192\n",
      "0.0025767663028091192\n",
      "0.0025150682777166367\n",
      "0.0025150682777166367\n",
      "0.0024550941307097673\n",
      "0.0024550941307097673\n",
      "0.0023961516562849283\n",
      "0.0023961516562849283\n",
      "0.0023390334099531174\n",
      "0.0023390334099531174\n",
      "0.00228294194675982\n",
      "0.00228294194675982\n",
      "0.002228685189038515\n",
      "0.002228685189038515\n",
      "0.0021753909531980753\n",
      "0.0021753909531980753\n",
      "0.002123467857018113\n",
      "0.002123467857018113\n",
      "0.00207248842343688\n",
      "0.00207248842343688\n",
      "0.002023054752498865\n",
      "0.002023054752498865\n",
      "0.0019747756887227297\n",
      "0.0019747756887227297\n",
      "0.001927632256411016\n",
      "0.001927632256411016\n",
      "0.0018816252704709768\n",
      "0.0018816252704709768\n",
      "0.0018366326112300158\n",
      "0.0018366326112300158\n",
      "0.0017929220339283347\n",
      "0.0017929220339283347\n",
      "0.0017502824775874615\n",
      "0.0017502824775874615\n",
      "0.0017084009014070034\n",
      "0.0017084009014070034\n",
      "0.0016673282952979207\n",
      "0.0016673282952979207\n",
      "0.0016275526722893119\n",
      "0.0016275526722893119\n",
      "0.0015886555192992091\n",
      "0.0015886555192992091\n",
      "0.0015511723468080163\n",
      "0.0015511723468080163\n",
      "0.0015141349285840988\n",
      "0.0015141349285840988\n",
      "0.0014779543271288276\n",
      "0.0014779543271288276\n",
      "0.0014425140107050538\n",
      "0.0014425140107050538\n",
      "0.0014083241112530231\n",
      "0.0014083241112530231\n",
      "0.0013747118646278977\n",
      "0.0013747118646278977\n",
      "0.001341681694611907\n",
      "0.001341681694611907\n",
      "0.001309900893829763\n",
      "0.001309900893829763\n",
      "0.0012784887803718448\n",
      "0.0012784887803718448\n",
      "0.0012479638680815697\n",
      "0.0012479638680815697\n",
      "0.0012182422215119004\n",
      "0.0012182422215119004\n",
      "0.001189161092042923\n",
      "0.001189161092042923\n",
      "0.0011609876528382301\n",
      "0.0011609876528382301\n",
      "0.0011333533329889178\n",
      "0.0011333533329889178\n",
      "0.0011062449775636196\n",
      "0.0011062449775636196\n",
      "0.001079855253919959\n",
      "0.001079855253919959\n",
      "0.0010541243245825171\n",
      "0.0010541243245825171\n",
      "0.001029020524583757\n",
      "0.001029020524583757\n",
      "0.0010046131210401654\n",
      "0.0010046131210401654\n",
      "0.0009806746384128928\n",
      "0.0009806746384128928\n",
      "0.000957195763476193\n",
      "0.000957195763476193\n",
      "0.0009345323778688908\n",
      "0.0009345323778688908\n",
      "0.0009121430339291692\n",
      "0.0009121430339291692\n",
      "0.0008905131253413856\n",
      "0.0008905131253413856\n",
      "0.0008693708223290741\n",
      "0.0008693708223290741\n",
      "0.0008487798040732741\n",
      "0.0008487798040732741\n",
      "0.0008285137009806931\n",
      "0.0008285137009806931\n",
      "0.0008088268805295229\n",
      "0.0008088268805295229\n",
      "0.000789559562690556\n",
      "0.000789559562690556\n",
      "0.000770875602029264\n",
      "0.000770875602029264\n",
      "0.000752566265873611\n",
      "0.000752566265873611\n",
      "0.0007349327788688242\n",
      "0.0007349327788688242\n",
      "0.000717450282536447\n",
      "0.000717450282536447\n",
      "0.000700240139849484\n",
      "0.000700240139849484\n",
      "0.0006836522952653468\n",
      "0.0006836522952653468\n",
      "0.0006674835458397865\n",
      "0.0006674835458397865\n",
      "0.0006515953573398292\n",
      "0.0006515953573398292\n",
      "0.0006360356346704066\n",
      "0.0006360356346704066\n",
      "0.0006209676503203809\n",
      "0.0006209676503203809\n",
      "0.0006062696920707822\n",
      "0.0006062696920707822\n",
      "0.0005919845425523818\n",
      "0.0005919845425523818\n",
      "0.0005778517224825919\n",
      "0.0005778517224825919\n",
      "0.0005642499891109765\n",
      "0.0005642499891109765\n",
      "0.0005509249167516828\n",
      "0.0005509249167516828\n",
      "0.0005379499634727836\n",
      "0.0005379499634727836\n",
      "0.0005251984694041312\n",
      "0.0005251984694041312\n",
      "0.0005127899930812418\n",
      "0.0005127899930812418\n",
      "0.0005005824496038258\n",
      "0.0005005824496038258\n",
      "0.0004886087263002992\n",
      "0.0004886087263002992\n",
      "0.0004771350941155106\n",
      "0.0004771350941155106\n",
      "0.0004658779944293201\n",
      "0.0004658779944293201\n",
      "0.0004547771532088518\n",
      "0.0004547771532088518\n",
      "0.00044405064545571804\n",
      "0.00044405064545571804\n",
      "0.00043354195076972246\n",
      "0.00043354195076972246\n",
      "0.0004234276129864156\n",
      "0.0004234276129864156\n",
      "0.0004134061455260962\n",
      "0.0004134061455260962\n",
      "0.0004036430618725717\n",
      "0.0004036430618725717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00039412069600075483\n",
      "0.00039412069600075483\n",
      "0.0003848111373372376\n",
      "0.0003848111373372376\n",
      "0.00037574523594230413\n",
      "0.00037574523594230413\n",
      "0.00036696248571388423\n",
      "0.00036696248571388423\n",
      "0.00035830913111567497\n",
      "0.00035830913111567497\n",
      "0.00034998220507986844\n",
      "0.00034998220507986844\n",
      "0.00034164093085564673\n",
      "0.00034164093085564673\n",
      "0.0003337223897688091\n",
      "0.0003337223897688091\n",
      "0.00032589404145255685\n",
      "0.00032589404145255685\n",
      "0.0003182206128258258\n",
      "0.0003182206128258258\n",
      "0.000310739764245227\n",
      "0.000310739764245227\n",
      "0.00030338275246322155\n",
      "0.00030338275246322155\n",
      "0.00029633622034452856\n",
      "0.00029633622034452856\n",
      "0.00028938104514963925\n",
      "0.00028938104514963925\n",
      "0.00028261810075491667\n",
      "0.00028261810075491667\n",
      "0.0002759683411568403\n",
      "0.0002759683411568403\n",
      "0.0002695330767892301\n",
      "0.0002695330767892301\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "# This make sure you get the same results returned in this snippet\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "prev_loss = 10000000\n",
    "max_error = 3\n",
    "for t in range(500):\n",
    "    #Make sure x, w1 & w2 are Variables\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute the loss with Mean Squared Error\n",
    "    loss = ((y_pred - y).pow(2).sum())/N\n",
    "    \n",
    "    print (loss.data[0])\n",
    "    if loss.data[0] > prev_loss:\n",
    "        max_error -= 1\n",
    "        if max_error <= 0:\n",
    "            print (\"SGD Error - Plase fix your Code your code!\")\n",
    "            break\n",
    "    else:\n",
    "        # Update max error allowed on loss fluctuation\n",
    "        max_error = 3\n",
    "    \n",
    "    # Update loss to check for correct implementation\n",
    "    prev_loss = loss.data[0]\n",
    "    \n",
    "    \n",
    "    # Zero the grad after the first step\n",
    "    # At the beginning they are None\n",
    "    if t > 1:\n",
    "        # Manually zero the gradients before running the backward pass\n",
    "        # CODE HERE - w1 grad must be zero-ed\n",
    "        # CODE HERE - w2 grad must be zero-ed\n",
    "       \n",
    "    # Print Loss per time step\n",
    "    print (loss.data[0])\n",
    "\n",
    "    # Auto differentation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent according the below formula\n",
    "    # (SGD) w = w - learning_rate * w_grad_wrt_loss\n",
    "    learning_rate = 1e-4\n",
    "    # CODE HERE - w1 SGD step update -> w1 = w1 - learning_rate * w1_grad_wrt_loss\n",
    "    # CODE HERE - w2 SGD step update -> w2 = w2 - learning_rate * w2_grad_wrt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://alykhantejani.github.io/images/gradient_descent_line_graph.gif\"/>\n",
    "</p>\n",
    "\n",
    "So until now, we've seen Tensors that hold the data, Variables wrap around Tensors to let them perform complex math operations & finally `autograd` to compute gradients. So why do these Variables need to retain a history of computation?\n",
    "\n",
    "The reason we wish to retain a computational graph of these variables is so we can differentiate & update them to optimize mathematical equations. This may not make much sense now, but hang on for a while. We'll get there. Say we have two Variables `y_` & `y`. `y_` is what our model predicts & `y` is what it **should** predict (remember supervised learning?).\n",
    "\n",
    "But how do we teach a machine that it's not doing a very good job of predicting `y` & needs to do better? You see, the basis of learning, be it biological beings like us or artificial machines, has always been 'repetition' of a  particular task i.e. a **learning algorithm**. To achieve this, we optimize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target y  Variable containing:\n",
      " 3\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Learned function, y_  Variable containing:\n",
      " 3.0032\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Optimization successful!\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "# This make sure you get the same results returned in this snippet\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Data and Label Variable\n",
    "x = Variable(torch.FloatTensor([5.0]), requires_grad=False)\n",
    "y = Variable(torch.FloatTensor([3.0]), requires_grad=False)\n",
    "\n",
    "# weight and bias\n",
    "w = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "b = Variable(torch.randn([1]), requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([w], lr=0.001)\n",
    "for i in range(100):\n",
    "    y_ = (x * w) + b\n",
    "    error = (y_ - y).abs() # Minimizes absolute difference\n",
    "    optimizer.zero_grad() # Zero the gradients before running the backward pass.\n",
    "    error.backward()      # Computes derivatives automatically\n",
    "    optimizer.step()      # Decreases loss: Updates y_ to become 'more' close to y\n",
    "    \n",
    "    \n",
    "print (\"Target y \", y)  # Evaluates to 3.0\n",
    "\n",
    "print (\"Learned function, y_ \", \n",
    "       y_,  # Evaluates to ~ 3.0 -- optimization successful!\n",
    "       \"\\nOptimization successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above snippet creates an optimizer called Stochastic Gradient Descent, passing it a list of parameters to optimize & a [learning rate](https://medium.com/@balamuralim.1993/importance-of-learning-rate-in-machine-learning-920a323fcbfb). We try to minimize the difference between `y_` & `y`, slowly. And after 100 steps, they become equal.\n",
    "\n",
    "We'll even use advanced optimizers like Adagrad & Adam when we get to Neural Nets. They're usually slower & more explanatory but are less likely to **overshoot** & thus, are used a lot. `torch.optim` module contains a number of these optimizers.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://2.bp.blogspot.com/-eW63YjSyuwY/V1QP3b9ZSmI/AAAAAAAAFeY/VcLfkmRvGaQbRjKhetlKjIl59kgkGV6hQCKgB/s1600/opt1.gif\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Trivia: Why are we doing this in PyTorch? Why not TensorFlow?\n",
    "\n",
    "You may skip this section & will still do fine, but it's interesting to know how exactly TensorFlow & PyTorch differ and how PyTorch is gaining so much popularity.\n",
    "\n",
    "With PyTorch & Tensorflow, being the two most comprehensive & popular frameworks, it didn't take much time to boil down our options to these two. Even though TensorFlow is more popular, we chose to go ahead with PyTorch for two primary reasons.\n",
    "\n",
    "**1. Graph Creation**: Creating & running graphs is where the two frameworks differ the most. Graphs in PyTorch are created dynamically, i.e at runtime. Whereas TensorFlow compiles the graph first, then executes it repeatedly. As a simple example, consider this:\n",
    "\n",
    "```python\n",
    "for _ in range(T):\n",
    "    h = torch.matmul(W, h) + b\n",
    "```\n",
    "\n",
    "Since the above operation takes place under a standard Python loop, `T` can be changed with each iteration of this code. TensorFlow on the other hand uses its [control flow operations](https://www.tensorflow.org/api_guides/python/control_flow_ops#Control_Flow_Operations) making it a bit too tedious to compute a graph dynamically. Furthermore, this makes debugging much easier. You'll see some more virtues of dynamic compute graphs in the upcoming articles.\n",
    "\n",
    "In TensorFlow, we define the [computate graph](https://www.tensorflow.org/programmers_guide/graphs) once and then execute the same graph over and over again, like a loop. In PyTorch, each forward pass defines a new computational graph.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.tensorflow.org/images/tensors_flowing.gif\"/>\n",
    "</p>\n",
    "\n",
    ">Credit: [TF Graph docs](https://www.tensorflow.org/programmers_guide/graphs)\n",
    "\n",
    ">Static graphs are nice because you can optimize the graph up front; framework might decide to fuse some graph\n",
    "operations for efficiency, or to come up with a strategy for distributing the graph across many GPUs or many\n",
    "machines. If you are reusing the same graph over and over, then this potentially costly up-front optimization can be amortized as the same graph is rerun over and over. However, for some models we may wish to perform\n",
    "different computations differently for each data point; for example a recurrent network might be unrolled for different numbers of time steps for each data point; this unrolling can be implemented as a loop. With a static graph the loop construct needs to be a part of the graph; for this reason TensorFlow provides operators such as tf.scan for embedding loops into the graph. With dynamic graphs the situation is simpler: since we build graphs on-the-fly for each example, we can use normal imperative flow control to perform computation that differs for each input.\n",
    "\n",
    "**2. Data Loaders**: With its well designed APIs, sampler & data loader, parallelizing data-flow operations is incredibly simple. TensorFlow provides us with some of its data loading tools (readers, queues, etc) but PyTorch is clearly miles ahead.\n",
    "\n",
    "*So why is TensorFlow so popular then?* While we may feel that learning about DL makes PyTorch a better candidate than TF, it may also be noted that there are certain fronts where TensorFlow does extremely well. Primarily in **Deployment**, **Device Management** & **Serialization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Exercise\n",
    "\n",
    "Fix the Optimization workflow for the next code snippet in the following way:\n",
    "- Define learning rate variable to 1e-4,\n",
    "- Define the Adam Optimizer passing 2 parameters: learning rate and model parameters,\n",
    "- Inside the loop, after computing the loss: zeros the grads, compute gradients and update the weights accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 632.82177734375\n",
      "1 616.0342407226562\n",
      "2 599.7182006835938\n",
      "3 583.8980102539062\n",
      "4 568.5601196289062\n",
      "5 553.7428588867188\n",
      "6 539.48974609375\n",
      "7 525.6441650390625\n",
      "8 512.2056274414062\n",
      "9 499.0415954589844\n",
      "10 486.1917419433594\n",
      "11 473.7044982910156\n",
      "12 461.57232666015625\n",
      "13 449.8550109863281\n",
      "14 438.465576171875\n",
      "15 427.3471984863281\n",
      "16 416.5296936035156\n",
      "17 406.01910400390625\n",
      "18 395.8365783691406\n",
      "19 385.93768310546875\n",
      "20 376.36749267578125\n",
      "21 367.14117431640625\n",
      "22 358.1838073730469\n",
      "23 349.5149230957031\n",
      "24 341.06573486328125\n",
      "25 332.82940673828125\n",
      "26 324.78094482421875\n",
      "27 316.93963623046875\n",
      "28 309.2872619628906\n",
      "29 301.8126220703125\n",
      "30 294.5040588378906\n",
      "31 287.3655090332031\n",
      "32 280.39263916015625\n",
      "33 273.5672912597656\n",
      "34 266.89208984375\n",
      "35 260.3569030761719\n",
      "36 253.98287963867188\n",
      "37 247.80311584472656\n",
      "38 241.78712463378906\n",
      "39 235.8996124267578\n",
      "40 230.1467742919922\n",
      "41 224.52374267578125\n",
      "42 219.02557373046875\n",
      "43 213.64776611328125\n",
      "44 208.37257385253906\n",
      "45 203.21932983398438\n",
      "46 198.16754150390625\n",
      "47 193.22544860839844\n",
      "48 188.40933227539062\n",
      "49 183.72549438476562\n",
      "50 179.15328979492188\n",
      "51 174.6881866455078\n",
      "52 170.31689453125\n",
      "53 166.03834533691406\n",
      "54 161.8574676513672\n",
      "55 157.75982666015625\n",
      "56 153.74658203125\n",
      "57 149.80557250976562\n",
      "58 145.9646453857422\n",
      "59 142.21163940429688\n",
      "60 138.54025268554688\n",
      "61 134.94590759277344\n",
      "62 131.4252166748047\n",
      "63 127.98854064941406\n",
      "64 124.62054443359375\n",
      "65 121.31449127197266\n",
      "66 118.086181640625\n",
      "67 114.92453002929688\n",
      "68 111.8318862915039\n",
      "69 108.81246948242188\n",
      "70 105.85348510742188\n",
      "71 102.95944213867188\n",
      "72 100.13106536865234\n",
      "73 97.35800170898438\n",
      "74 94.6377182006836\n",
      "75 91.97966003417969\n",
      "76 89.37449645996094\n",
      "77 86.8216323852539\n",
      "78 84.32553100585938\n",
      "79 81.88740539550781\n",
      "80 79.50410461425781\n",
      "81 77.17188262939453\n",
      "82 74.8890609741211\n",
      "83 72.65930938720703\n",
      "84 70.48416900634766\n",
      "85 68.365234375\n",
      "86 66.29805755615234\n",
      "87 64.28379821777344\n",
      "88 62.31952667236328\n",
      "89 60.40306091308594\n",
      "90 58.529090881347656\n",
      "91 56.7003059387207\n",
      "92 54.918914794921875\n",
      "93 53.177711486816406\n",
      "94 51.48029327392578\n",
      "95 49.82795333862305\n",
      "96 48.21687316894531\n",
      "97 46.64885711669922\n",
      "98 45.124366760253906\n",
      "99 43.63722229003906\n",
      "100 42.18793869018555\n",
      "101 40.7791862487793\n",
      "102 39.40806579589844\n",
      "103 38.07530975341797\n",
      "104 36.778812408447266\n",
      "105 35.5202522277832\n",
      "106 34.29642105102539\n",
      "107 33.10688781738281\n",
      "108 31.951414108276367\n",
      "109 30.829132080078125\n",
      "110 29.740154266357422\n",
      "111 28.681289672851562\n",
      "112 27.654611587524414\n",
      "113 26.657276153564453\n",
      "114 25.690507888793945\n",
      "115 24.751182556152344\n",
      "116 23.839096069335938\n",
      "117 22.95395278930664\n",
      "118 22.09514617919922\n",
      "119 21.263565063476562\n",
      "120 20.456928253173828\n",
      "121 19.675373077392578\n",
      "122 18.919063568115234\n",
      "123 18.186288833618164\n",
      "124 17.476778030395508\n",
      "125 16.790563583374023\n",
      "126 16.124752044677734\n",
      "127 15.481049537658691\n",
      "128 14.857810020446777\n",
      "129 14.256064414978027\n",
      "130 13.67470645904541\n",
      "131 13.11343765258789\n",
      "132 12.57101058959961\n",
      "133 12.04689884185791\n",
      "134 11.54171085357666\n",
      "135 11.054574012756348\n",
      "136 10.584606170654297\n",
      "137 10.131397247314453\n",
      "138 9.69532585144043\n",
      "139 9.275125503540039\n",
      "140 8.870153427124023\n",
      "141 8.481338500976562\n",
      "142 8.108232498168945\n",
      "143 7.747660160064697\n",
      "144 7.401622772216797\n",
      "145 7.0686936378479\n",
      "146 6.749332904815674\n",
      "147 6.442570209503174\n",
      "148 6.148166656494141\n",
      "149 5.865750789642334\n",
      "150 5.595296382904053\n",
      "151 5.335840702056885\n",
      "152 5.087377071380615\n",
      "153 4.8492326736450195\n",
      "154 4.620766639709473\n",
      "155 4.401976108551025\n",
      "156 4.192462921142578\n",
      "157 3.9920756816864014\n",
      "158 3.8003952503204346\n",
      "159 3.617204427719116\n",
      "160 3.441786527633667\n",
      "161 3.2741003036499023\n",
      "162 3.1140518188476562\n",
      "163 2.9607884883880615\n",
      "164 2.8144993782043457\n",
      "165 2.674941301345825\n",
      "166 2.5416419506073\n",
      "167 2.4144482612609863\n",
      "168 2.2931432723999023\n",
      "169 2.177375316619873\n",
      "170 2.0671496391296387\n",
      "171 1.9619189500808716\n",
      "172 1.8617017269134521\n",
      "173 1.7661755084991455\n",
      "174 1.6754282712936401\n",
      "175 1.5890201330184937\n",
      "176 1.5066819190979004\n",
      "177 1.4285330772399902\n",
      "178 1.3540383577346802\n",
      "179 1.2831435203552246\n",
      "180 1.2157528400421143\n",
      "181 1.1517006158828735\n",
      "182 1.0907108783721924\n",
      "183 1.0327969789505005\n",
      "184 0.9777162075042725\n",
      "185 0.9255729913711548\n",
      "186 0.8760133385658264\n",
      "187 0.8288878798484802\n",
      "188 0.7842487692832947\n",
      "189 0.7418367862701416\n",
      "190 0.701600193977356\n",
      "191 0.6634883880615234\n",
      "192 0.6272624731063843\n",
      "193 0.5929530262947083\n",
      "194 0.5604338049888611\n",
      "195 0.5296482443809509\n",
      "196 0.5004376769065857\n",
      "197 0.4728313684463501\n",
      "198 0.4466761350631714\n",
      "199 0.4219105839729309\n",
      "200 0.3984440267086029\n",
      "201 0.37625375390052795\n",
      "202 0.355253130197525\n",
      "203 0.33539238572120667\n",
      "204 0.31660839915275574\n",
      "205 0.2988331615924835\n",
      "206 0.28202277421951294\n",
      "207 0.26613059639930725\n",
      "208 0.25112318992614746\n",
      "209 0.23691613972187042\n",
      "210 0.22352614998817444\n",
      "211 0.21085260808467865\n",
      "212 0.19886037707328796\n",
      "213 0.18756060302257538\n",
      "214 0.17688649892807007\n",
      "215 0.16679605841636658\n",
      "216 0.15727359056472778\n",
      "217 0.14829021692276\n",
      "218 0.13981446623802185\n",
      "219 0.13179101049900055\n",
      "220 0.12422871589660645\n",
      "221 0.11709745228290558\n",
      "222 0.11035893857479095\n",
      "223 0.10400740057229996\n",
      "224 0.09800899773836136\n",
      "225 0.09236014634370804\n",
      "226 0.08702314645051956\n",
      "227 0.08199097216129303\n",
      "228 0.07725659012794495\n",
      "229 0.07277235388755798\n",
      "230 0.06855390965938568\n",
      "231 0.0645752027630806\n",
      "232 0.06082310900092125\n",
      "233 0.057286765426397324\n",
      "234 0.05395498499274254\n",
      "235 0.05080994591116905\n",
      "236 0.047848012298345566\n",
      "237 0.04505801945924759\n",
      "238 0.04242926836013794\n",
      "239 0.03995339199900627\n",
      "240 0.037615228444337845\n",
      "241 0.03541379049420357\n",
      "242 0.03334055095911026\n",
      "243 0.03138440474867821\n",
      "244 0.02954440750181675\n",
      "245 0.027812479063868523\n",
      "246 0.026176879182457924\n",
      "247 0.02464122325181961\n",
      "248 0.02319379709661007\n",
      "249 0.02182702347636223\n",
      "250 0.020541870966553688\n",
      "251 0.01933247409760952\n",
      "252 0.018193179741501808\n",
      "253 0.017119137570261955\n",
      "254 0.016108563169836998\n",
      "255 0.015157020650804043\n",
      "256 0.01426047831773758\n",
      "257 0.0134162288159132\n",
      "258 0.012622125446796417\n",
      "259 0.01187314186245203\n",
      "260 0.011168449185788631\n",
      "261 0.010505015030503273\n",
      "262 0.009884776547551155\n",
      "263 0.009302915073931217\n",
      "264 0.008753741160035133\n",
      "265 0.00823704618960619\n",
      "266 0.007750913500785828\n",
      "267 0.007293278817087412\n",
      "268 0.006862599402666092\n",
      "269 0.006457251962274313\n",
      "270 0.006075696554034948\n",
      "271 0.005716688930988312\n",
      "272 0.0053785708732903\n",
      "273 0.005060588475316763\n",
      "274 0.004761355463415384\n",
      "275 0.004479179158806801\n",
      "276 0.004214025568217039\n",
      "277 0.003964264877140522\n",
      "278 0.0037290523760020733\n",
      "279 0.003507673041895032\n",
      "280 0.00329926167614758\n",
      "281 0.0031030529644340277\n",
      "282 0.00291837053373456\n",
      "283 0.002744543133303523\n",
      "284 0.0025808680802583694\n",
      "285 0.0024268270935863256\n",
      "286 0.002281902590766549\n",
      "287 0.002145505743101239\n",
      "288 0.002017054008319974\n",
      "289 0.0018960891757160425\n",
      "290 0.0017822803929448128\n",
      "291 0.001675176783464849\n",
      "292 0.0015744196716696024\n",
      "293 0.0014796608593314886\n",
      "294 0.0013903840444982052\n",
      "295 0.0013064418453723192\n",
      "296 0.0012274441542103887\n",
      "297 0.0011531836353242397\n",
      "298 0.0010832886910066009\n",
      "299 0.0010175297502428293\n",
      "300 0.0009556681616231799\n",
      "301 0.0008975277305580676\n",
      "302 0.0008427947759628296\n",
      "303 0.0007913736626505852\n",
      "304 0.0007430515834130347\n",
      "305 0.0006975122960284352\n",
      "306 0.0006547637167386711\n",
      "307 0.0006145625957287848\n",
      "308 0.0005767804686911404\n",
      "309 0.0005412548780441284\n",
      "310 0.0005078945541754365\n",
      "311 0.00047653194633312523\n",
      "312 0.0004470506391953677\n",
      "313 0.00041934801265597343\n",
      "314 0.00039333110908046365\n",
      "315 0.00036888918839395046\n",
      "316 0.00034592708107084036\n",
      "317 0.0003243617538828403\n",
      "318 0.0003041100571863353\n",
      "319 0.0002850822638720274\n",
      "320 0.00026721516042016447\n",
      "321 0.0002504806616343558\n",
      "322 0.00023471321037504822\n",
      "323 0.00021994276903569698\n",
      "324 0.0002060732658719644\n",
      "325 0.0001930496218847111\n",
      "326 0.00018083074246533215\n",
      "327 0.00016938387125264853\n",
      "328 0.00015862632426433265\n",
      "329 0.00014854452456347644\n",
      "330 0.0001390840916428715\n",
      "331 0.00013021152699366212\n",
      "332 0.00012188540858915076\n",
      "333 0.00011408445425331593\n",
      "334 0.00010676782403606921\n",
      "335 9.99144249362871e-05\n",
      "336 9.347934974357486e-05\n",
      "337 8.745697414269671e-05\n",
      "338 8.180830627679825e-05\n",
      "339 7.651055057067424e-05\n",
      "340 7.15542264515534e-05\n",
      "341 6.691247835988179e-05\n",
      "342 6.255799962673336e-05\n",
      "343 5.848281580256298e-05\n",
      "344 5.466476795845665e-05\n",
      "345 5.108846380608156e-05\n",
      "346 4.774382250616327e-05\n",
      "347 4.460784111870453e-05\n",
      "348 4.167492443230003e-05\n",
      "349 3.893018947564997e-05\n",
      "350 3.635965913417749e-05\n",
      "351 3.395601379452273e-05\n",
      "352 3.170866330037825e-05\n",
      "353 2.9605836971313693e-05\n",
      "354 2.763684096862562e-05\n",
      "355 2.5798106435104273e-05\n",
      "356 2.4076809495454654e-05\n",
      "357 2.2467247617896646e-05\n",
      "358 2.096142270602286e-05\n",
      "359 1.9556544430088252e-05\n",
      "360 1.8243086742586456e-05\n",
      "361 1.7014439436024986e-05\n",
      "362 1.5867748516029678e-05\n",
      "363 1.4794464732403867e-05\n",
      "364 1.379382774757687e-05\n",
      "365 1.285642520088004e-05\n",
      "366 1.1984497177763842e-05\n",
      "367 1.1167679076606873e-05\n",
      "368 1.0408081834611949e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369 9.696146662463434e-06\n",
      "370 9.032905836647842e-06\n",
      "371 8.413609066337813e-06\n",
      "372 7.83595532993786e-06\n",
      "373 7.2980446930159815e-06\n",
      "374 6.793623470002785e-06\n",
      "375 6.32607134321006e-06\n",
      "376 5.888831310585374e-06\n",
      "377 5.480385425471468e-06\n",
      "378 5.09972733198083e-06\n",
      "379 4.7456114771193825e-06\n",
      "380 4.414420800458174e-06\n",
      "381 4.106736923858989e-06\n",
      "382 3.820125130005181e-06\n",
      "383 3.5527482395991683e-06\n",
      "384 3.3038259061868303e-06\n",
      "385 3.071978881052928e-06\n",
      "386 2.8547617603180697e-06\n",
      "387 2.6542174964561127e-06\n",
      "388 2.4662101623107446e-06\n",
      "389 2.2923966298549203e-06\n",
      "390 2.129453378074686e-06\n",
      "391 1.9782974050031044e-06\n",
      "392 1.8377932065050118e-06\n",
      "393 1.706218313302088e-06\n",
      "394 1.5849684587010415e-06\n",
      "395 1.4718700640514726e-06\n",
      "396 1.3664537164004287e-06\n",
      "397 1.2686374475379125e-06\n",
      "398 1.1771192021114985e-06\n",
      "399 1.0928218898698105e-06\n",
      "400 1.0136735681953724e-06\n",
      "401 9.404033107784926e-07\n",
      "402 8.723727091819455e-07\n",
      "403 8.088707659226202e-07\n",
      "404 7.506167207793624e-07\n",
      "405 6.958796348044416e-07\n",
      "406 6.450922569456452e-07\n",
      "407 5.981251547382271e-07\n",
      "408 5.544580972127733e-07\n",
      "409 5.138289793649164e-07\n",
      "410 4.7621182375223725e-07\n",
      "411 4.4103404661655077e-07\n",
      "412 4.086208491571597e-07\n",
      "413 3.785206388329243e-07\n",
      "414 3.5065846759607666e-07\n",
      "415 3.2449869991069136e-07\n",
      "416 3.005661426414008e-07\n",
      "417 2.7829136683976685e-07\n",
      "418 2.576128395048727e-07\n",
      "419 2.3829328199553856e-07\n",
      "420 2.2054840087548655e-07\n",
      "421 2.040896873722886e-07\n",
      "422 1.8856405858969083e-07\n",
      "423 1.7454887313306244e-07\n",
      "424 1.613500870689677e-07\n",
      "425 1.4920789226380293e-07\n",
      "426 1.3799633791222732e-07\n",
      "427 1.2743949184823578e-07\n",
      "428 1.1787549425434918e-07\n",
      "429 1.088662315851252e-07\n",
      "430 1.0047529030998703e-07\n",
      "431 9.28484951145947e-08\n",
      "432 8.577142551757788e-08\n",
      "433 7.923694056444219e-08\n",
      "434 7.321730777221092e-08\n",
      "435 6.757255022193931e-08\n",
      "436 6.231341842521942e-08\n",
      "437 5.75732030938525e-08\n",
      "438 5.310131712121802e-08\n",
      "439 4.8977710775943706e-08\n",
      "440 4.523322516547523e-08\n",
      "441 4.1707746589736416e-08\n",
      "442 3.8466740903686514e-08\n",
      "443 3.540579740501926e-08\n",
      "444 3.2654408954613245e-08\n",
      "445 3.0132145667494115e-08\n",
      "446 2.779272634256813e-08\n",
      "447 2.563553280765518e-08\n",
      "448 2.3621129940920582e-08\n",
      "449 2.180666314188784e-08\n",
      "450 2.0101094122537688e-08\n",
      "451 1.8534201728925837e-08\n",
      "452 1.706337826590243e-08\n",
      "453 1.574441554907935e-08\n",
      "454 1.4504509593393777e-08\n",
      "455 1.3369596985057797e-08\n",
      "456 1.2329910425989965e-08\n",
      "457 1.1369666985672211e-08\n",
      "458 1.0445505793654775e-08\n",
      "459 9.619529173221508e-09\n",
      "460 8.864650347106817e-09\n",
      "461 8.160722764216644e-09\n",
      "462 7.498731413591031e-09\n",
      "463 6.945588992834928e-09\n",
      "464 6.4068448324405836e-09\n",
      "465 5.905724798083156e-09\n",
      "466 5.455086160566225e-09\n",
      "467 5.019074933443335e-09\n",
      "468 4.645368534283989e-09\n",
      "469 4.277251441919816e-09\n",
      "470 3.950050952994388e-09\n",
      "471 3.6404477210538744e-09\n",
      "472 3.359934330759984e-09\n",
      "473 3.125434355766288e-09\n",
      "474 2.880208738176293e-09\n",
      "475 2.680246691255661e-09\n",
      "476 2.494545903175549e-09\n",
      "477 2.3130797277559623e-09\n",
      "478 2.159639800325408e-09\n",
      "479 1.999997722990088e-09\n",
      "480 1.8529413559065233e-09\n",
      "481 1.7213210856681371e-09\n",
      "482 1.60674873406208e-09\n",
      "483 1.4955281457673664e-09\n",
      "484 1.3896752637521104e-09\n",
      "485 1.3036638435437453e-09\n",
      "486 1.221284184893534e-09\n",
      "487 1.134272009650772e-09\n",
      "488 1.0672196459893257e-09\n",
      "489 1.001605243189374e-09\n",
      "490 9.38071176292965e-10\n",
      "491 8.892257485904054e-10\n",
      "492 8.232420301013121e-10\n",
      "493 7.802305468374016e-10\n",
      "494 7.291964254640959e-10\n",
      "495 6.853670964090952e-10\n",
      "496 6.510330052833524e-10\n",
      "497 6.125200346929205e-10\n",
      "498 5.763221566645882e-10\n",
      "499 5.473616560003336e-10\n"
     ]
    }
   ],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Load variables on GPU\n",
    "if cuda:\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "    \n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "# Load model and loss_fn on GPU\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    loss_fn.cuda()\n",
    "\n",
    "#### DEFINE OPTIMIZER ####\n",
    "# Define learning rate\n",
    "learning_rate = # CODE HERE\n",
    "# Init Adam with parameters(model params and learning rate)\n",
    "optimizer = # CODE HERE\n",
    "\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y by passing x to the model.\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Compute and print loss.\n",
    "  loss = loss_fn(y_pred, y)\n",
    "  print(t, loss.data[0])\n",
    "  \n",
    "  #### BACKPROP ####\n",
    "  # Zero the grads\n",
    "  # CODE HERE\n",
    "\n",
    "  # Backward pass\n",
    "  # CODE HERE\n",
    "\n",
    "  # Update the weight with Adam\n",
    "  # CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up: Handwritten Digit Classification\n",
    "\n",
    "So that's all for now. For the next article in this series, we are introducing a classical problem in Computer Vision: Handwritten Digit Recognition. Until now we've seen how to use Tensors (n-dimensional arrays) in PyTorch & compute their gradients with Autograd. The handwritten digit recognition is an example of a **classification** problem; given an image of a digit we can to classify it as either 0, 1, 2, 3...9. Each digit to be classified is known as a class. We will (try) to build a classifier with only whatever you've learned until now & then finally introduce you to the Artificial Neural Networks.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/sominwadhwa/sominwadhwa.github.io/blob/master/assets/intro_to_pytorch_series/mnist_logreg.jpeg?raw=true\"/>\n",
    "</p>\n",
    "\n",
    "Task: we'll be given a greyscale image (28 x 28) of some handwritten digit. We'll process this image to get a 28 x 28 matrix of real valued numbers, called **features** of this image. Our objective would be to **map a relationship between these features & the probability of a particular outcome**. Before moving on to the next article, if you are not familiar with this kind of a task, or wish to seek a quick intro to Logistic Regression, give [this article](https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389) a quick 5 minute read & you're good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For this task we will use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. We've already uploaded the entire [dataset on FloydHub](https://www.floydhub.com/redeipirati/datasets/mnist) & you can access the same via the `input` path.\n",
    "\n",
    "To learn how datasets are managed on FloydHub, you can checkout the [dataset documentation](https://docs.floydhub.com/guides/create_and_upload_dataset/) or checkout this quick [tutorial](https://blog.floydhub.com/getting-started-with-deep-learning-on-floydhub/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "PyTorch provides an amazing framework with an awesome community that can support us in our DL journey. We introduced PyTorch & in the next article you'll some more traditional use cases of PyTorch; We'll be implementing a full scale `Classification` exercise on PyTorch using Logistic Regression, look for some improvements through a single layer Neural Network as well as create some more 'strange' networks to give you a good idea how Dynamic Compute graphs make PyTorch so powerful.\n",
    "\n",
    "*Note:* You should know that the PyTorch's [documentation](http://pytorch.org/docs/master/) and [tutorials](http://pytorch.org/tutorials/) are stored separately. And sometimes they may not converge due to the rapid speed of development and version changes. So feel free to investigate the [source code](https://github.com/pytorch/pytorch), if you feel so. [PyTorch Forums](https://discuss.pytorch.org/) are another great place to get your doubts cleared up. If you do however have any doubts/queries regarding our examples or in general, do let us know on the, we'll be happy to help.\n",
    "\n",
    "We hope you enjoyed this Introduction to PyTorch. If you'd like to share your feedback (cheers, bug fix, typo and/or improvements), please leave us a comment on our super active [forum](https://forum.floydhub.com/) or tweet us [@FloydHub_](https://twitter.com/FloydHub_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "**Big thanks** to:\n",
    " - [Illarion Khlestov](https://medium.com/@illarionkhlestov) for the code snippets & images.\n",
    " - [PyTorch](http://pytorch.org/tutorials/) for the docs, code snippets, images and the amazing framework.\n",
    " - [Justin Johnson](http://cs.stanford.edu/people/jcjohns/) for the pytorch examples and snippets of code.\n",
    "\n",
    "Link References:\n",
    " - Pytorch [docs](http://pytorch.org/docs/master/) and [tutorial](http://pytorch.org/tutorials/)\n",
    " - [jcjohnson pytorch examples](https://github.com/jcjohnson/pytorch-examples)\n",
    " - [PyTorch tutorial distilled by Illarion Khlestov](https://medium.com/towards-data-science/pytorch-tutorial-distilled-95ce8781a89c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

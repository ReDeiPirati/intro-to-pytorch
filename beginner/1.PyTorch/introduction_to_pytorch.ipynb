{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FloydHub Introduction to Deep Learning: PyTorch\n",
    "\n",
    "<img style=\"align: center;height: 300px; widht: 300px;\" src=\"https://github.com/sominwadhwa/sominwadhwa.github.io/blob/master/assets/intro_to_pytorch_series/PyTorch.jpg?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[PyTorch](http://pytorch.org/) is one among the numerous [Deep Learning frameworks](https://www.kdnuggets.com/2017/02/python-deep-learning-frameworks-overview.html) which allows us to build powerful Deep Learning models by harnessing GPU compute. PyTorch is extensively used for rapid prototyping in research and small scale projects. The objective of this article is to give you a hands on experience with PyTorch & some basic mathematical lingo associated with Deep Learning. We also introduce the classic problem of [Handwritten Digit Recognition](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "**Table of Contents**:\n",
    "\n",
    "- [PyTorch Introduction](#pytorch-introduction)\n",
    "- [Tensors](#tensor)\n",
    "- [Variables & Autograd](#variables-and-autograd)\n",
    "- [Logistic Regression](#logistic-regression)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Introduction\n",
    "\n",
    "PyTorch is a Python based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A Deep Learning research platform that provides maximum flexibility and speed through Dynamic Compute graphs & Imperative Programming control flow.\n",
    "- A replacement for NumPy to harness GPU compute capability.\n",
    "\n",
    "Here's a list of modules we'll need in order to run this tutorial:\n",
    "\n",
    "1. [torch.autograd](http://pytorch.org/docs/master/autograd.html) Provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\n",
    "2. [torch.nn](http://pytorch.org/docs/master/nn.html) Package provides an easy and modular way to build and train simple or complex neural networks.\n",
    "3. [torchvision](http://pytorch.org/docs/master/torchvision/index.html) consists of popular datasets, model architectures & common image transformations.\n",
    "4. [NumPy](http://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "\n",
    "#### Running Code Cell\n",
    "\n",
    "If you want to run Cell code using shortcut, type **`shift + enter`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the package we need to run the tutorial\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Is CUDA available on this instance?\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Seed for reproducibility\n",
    "# This make sure you get the same results returned in this notebook\n",
    "torch.manual_seed(1)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "In any deep learning pipeline, one obvious inevitable thing that we encounter, is mathematical data. Be it an images stored in the form of `[height x width]` matrices, a piece of text stored in the form a vector or some spooky operation taking place between those two. PyTorch provides us with objects known as Tensors that store all this data under one roof.\n",
    "\n",
    "*Formally*, a [PyTorch Tensor](http://pytorch.org/docs/master/tensors.html) is conceptually identical to a NumPy's `ndarray`, and PyTorch provides many functions for operating on these Tensors. Like standard `ndarrays`, PyTorch Tensors do not know anything about deep learning or computational graphs or gradients; they are a generic tool for scientific computing. We can use n-dimensional Tensors to our requirement, for instance - we can have multidimensional (2D) tensor storing an image, or a single variable storing text.\n",
    "\n",
    "The following snippets demonstrate Tensors & a few of their operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Tensor(5, 3):\n",
      "\n",
      " 6.3480e+01  4.5577e-41  9.1041e-38\n",
      " 0.0000e+00 -1.4170e+23  4.5576e-41\n",
      " 4.5092e-22  4.5577e-41  3.7845e-05\n",
      " 4.5577e-41 -1.3773e+23  4.5576e-41\n",
      " 3.7939e-05  4.5577e-41  0.0000e+00\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Tensor on GPU if CUDA available\n",
    "dtype = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# Construct a 5x3 matrix, uninitialized:\n",
    "print(\"torch.Tensor(5, 3):\")\n",
    "x = torch.Tensor(5, 3).type(dtype)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.rand(5, 3):\n",
      "\n",
      " 0.4170  0.9972  0.7203\n",
      " 0.9326  0.0001  0.1281\n",
      " 0.3023  0.9990  0.1468\n",
      " 0.2361  0.0923  0.3966\n",
      " 0.1863  0.3879  0.3456\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct a randomly initialized matrix\n",
    "print(\"torch.rand(5, 3):\")\n",
    "x = torch.rand(5, 3).type(dtype)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Tensor Size:\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# Get its size\n",
    "print(\"Last Tensor Size:\")\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax 1: x + y =\n",
      "\n",
      " 1.0868  1.3940  1.6559\n",
      " 1.4714  0.8464  0.5473\n",
      " 0.6156  1.6843  0.6713\n",
      " 0.4405  0.5358  1.2747\n",
      " 0.4158  0.4153  0.8800\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are multiple syntaxes for operations. Let’s see addition as an example\n",
    "# Addition: syntax 1\n",
    "y = torch.rand(5, 3)\n",
    "print(\"Syntax 1: x + y =\")\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax 2: torch.add(x, y) =\n",
      "\n",
      " 1.0868  1.3940  1.6559\n",
      " 1.4714  0.8464  0.5473\n",
      " 0.6156  1.6843  0.6713\n",
      " 0.4405  0.5358  1.2747\n",
      " 0.4158  0.4153  0.8800\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition: syntax 2\n",
    "print(\"Syntax 2: torch.add(x, y) =\")\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax 3: torch.add(x, y, out=result) =\n",
      "\n",
      " 1.0868  1.3940  1.6559\n",
      " 1.4714  0.8464  0.5473\n",
      " 0.6156  1.6843  0.6713\n",
      " 0.4405  0.5358  1.2747\n",
      " 0.4158  0.4153  0.8800\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition: giving an output tensor\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(\"Syntax 3: torch.add(x, y, out=result) =\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-place Addition: y.add_(x) =\n",
      "\n",
      " 1.0868  1.3940  1.6559\n",
      " 1.4714  0.8464  0.5473\n",
      " 0.6156  1.6843  0.6713\n",
      " 0.4405  0.5358  1.2747\n",
      " 0.4158  0.4153  0.8800\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition: in-place\n",
    "# adds x to y\n",
    "print(\"In-place Addition: y.add_(x) =\")\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing x[:, 1] - Second column(index starts from zero) of every rows:\n",
      "\n",
      " 0.9972\n",
      " 0.0001\n",
      " 0.9990\n",
      " 0.0923\n",
      " 0.3879\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can use standard numpy-like indexing with all bells and whistles!\n",
    "print (\"Indexing x[:, 1] - Second column(index starts from zero) of every rows:\")\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NumPy `ndarrays`, PyTorch Tensors can utilize GPUs to accelerate their numeric computations & PyTorch makes it ridiculously easy to switch from GPU to CPU & vice versa.\n",
    "\n",
    "*Note: It is interesting to know that PyTorch can serve as a full fledged replacement for NumPy, as Tensors & ndarrays can be used interchangeably. You can checkout the ipython notebook for an implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy tensor:  [[-2.02919185 -0.49105739 -0.51260478  0.59240281]\n",
      " [ 0.71304078  0.07126919  1.68042955 -0.63858111]\n",
      " [-0.49939581 -1.04035265  1.51937907 -0.12309448]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a sample matrix of 3 rows and 4 columns \n",
    "# from a Normal Distribution with Mean 0 and Var 1 \n",
    "numpy_tensor = np.random.randn(3, 4)\n",
    "print (\"Numpy tensor: \", numpy_tensor, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy to PyTorch tensor:  \n",
      "-2.0292 -0.4911 -0.5126  0.5924\n",
      " 0.7130  0.0713  1.6804 -0.6386\n",
      "-0.4994 -1.0404  1.5194 -0.1231\n",
      "[torch.FloatTensor of size 3x4]\n",
      " \n",
      "\n",
      "PyTorch to Numpy tensor:  [[-2.02919185 -0.49105739 -0.51260478  0.59240281]\n",
      " [ 0.71304078  0.07126919  1.68042955 -0.63858111]\n",
      " [-0.49939581 -1.04035265  1.51937907 -0.12309448]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy array to pytorch array\n",
    "pytorch_tensor = torch.Tensor(numpy_tensor)\n",
    "print (\"Numpy to PyTorch tensor: \", pytorch_tensor, \"\\n\")\n",
    "# Or another way\n",
    "pytorch_tensor = torch.from_numpy(numpy_tensor)\n",
    "# Convert torch tensor to numpy representation\n",
    "print (\"PyTorch to Numpy tensor: \", pytorch_tensor.numpy(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: you can run the GPU to CPU example only if you are running a FloydHub GPU instance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If cuda is available, run GPU-to-CPU and vice versa example\n",
    "if cuda:\n",
    "    # If we want to use tensor on GPU provide another type\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    gpu_tensor = torch.randn(5, 10).type(dtype)\n",
    "    # Or just call `cuda()` method\n",
    "    gpu_tensor = pytorch_tensor.cuda()\n",
    "    print (\"PyTorch cuda gpu_tensor \", gpu_tensor, \"\\n\")\n",
    "    # Call back to the CPU\n",
    "    cpu_tensor = gpu_tensor.cpu()\n",
    "    print (\"PyTorch cuda tensor to cpu_tensor, gpu_tensor.cpu() \", cpu_tensor, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define pytorch tensors\n",
    "x = torch.randn(10, 20)\n",
    "y = torch.ones(20, 5)\n",
    "# `@` mean matrix multiplication from python3.5, PEP-0465\n",
    "res = x @ y # Same as torch.matmul(x, y)\n",
    "\n",
    "# Get the shape\n",
    "res.shape  # torch.Size([10, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Exercise\n",
    "\n",
    "Create a square Matrix as this: [[1, 2], [3, 4]], multiply it for this *column* vector [5, 6] and add at the result the following column vector [7, 8]. The result should be a column vector with these values: [24, 47].\n",
    "\n",
    "`result = matrix * column1 + column2`\n",
    "\n",
    "*Note: There is more than one way to achieve the same result.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 24\n",
      " 47\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a 2-D Tensor with these values [[1, 2], [3, 4]]\n",
    "matrix = ...\n",
    "\n",
    "# Create a column Vector with these values [5, 6]\n",
    "column1 = ...\n",
    "\n",
    "# Create a column Vector with these values [7, 8]\n",
    "column2 = ...\n",
    "\n",
    "# Matrix Mult\n",
    "mutl = ...\n",
    "\n",
    "# Addition\n",
    "res = ...\n",
    "\n",
    "# Show the result\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and AutoGrad\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://pytorch.org/tutorials/_images/Variable.png\"/>\n",
    "</p>\n",
    "\n",
    ">Credits: PyTorch Variable Docs\n",
    "\n",
    "Variables are **wrappers** over Tensors that allow them to be differentiated & modified. Let me demonstrate how: Take the example in the following snippet, where we apply a string of operations over a 'Variable' `x`, to predict `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "x + 2 = y, Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      " \n",
      "\n",
      "y was created as a result of an operation, so we have  <torch.autograd.function.AddConstantBackward object at 0x7f0d300c0e58> \n",
      "\n",
      "y * y * * 3 = z Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " \n",
      " mean(z),  Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "After backprop, x Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Var and Authograd example on simple operations\n",
    "\n",
    "# Create a Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(\"x\", x)\n",
    "\n",
    "# Make an op\n",
    "y = x + 2\n",
    "print(\"x + 2 = y,\", y, \"\\n\")\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn.\n",
    "print(\"y was created as a result of an operation, so we have \", y.grad_fn, \"\\n\")\n",
    "\n",
    "# More op\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(\"y * y * * 3 = z\", z, \"\\n\", \"mean(z), \", out)\n",
    "\n",
    "# Let’s backprop now \n",
    "out.backward()\n",
    "print(\"After backprop, x\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 17\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Variable objects containing non-empty torch.ByteTensor is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5bb6339f525a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mh_relu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_relu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#y_pred = h_relu.dot(w2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         raise RuntimeError(\"bool value of Variable objects containing non-empty \" +\n\u001b[0;32m--> 123\u001b[0;31m                            torch.typename(self.data) + \" is ambiguous\")\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Variable objects containing non-empty torch.ByteTensor is ambiguous"
     ]
    }
   ],
   "source": [
    "# x, w1, w2 is a PyTorch Variable\n",
    "x = Variable(torch.Tensor([1, 2]))\n",
    "w1 = Variable(torch.Tensor([5, 6]))\n",
    "w2 = Variable(torch.Tensor([[5, 6], [7, 8]]))\n",
    "\n",
    "# Computes the dot product (inner product) of two tensors \n",
    "h = x.dot(w1)\n",
    "print (h)\n",
    "h_relu = np.maximum(h,0)\n",
    "print (h_relu)\n",
    "#y_pred = h_relu.dot(w2)\n",
    "\n",
    "#loss = np.mean(np.square(y_pred - y).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we wish to compute the derivative of this function with respect to the loss. Using the traditional symbolic differentiation, we would achieve that in a way like this,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Gradient\n",
    "grad_y_pred = 2.0*(y_pred - y)\n",
    "grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "grad_h = grad_h_relu.copy()\n",
    "grad_h[h < 0] = 0\n",
    "grad_w1 = x.T.dot(grad_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This process mentioned up is a part of Backpropagation in a simple single layer Neural Network, don't worry about it even if you don't understand much of it, we'll cover it in the next article.\n",
    "\n",
    "Now imagine, if there were tens of different types of mathematical operations before computing `loss` in the first snippet (because there will be in what's about to come!). How could you possibly code the gradient computation for something like that? Thankfully, `torch.autograd` exists. It works on the principle of Automatic Differentiation, which is inherently based on the **chain rule**. To perform the gradient computation in the above example using `autograd`, all we have to do is,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure x, w1 & w2 are Variables\n",
    "y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "loss = np.square(y_pred - y).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every variable instance has two attributes: `.data` that contains the initial tensor itself and `.grad` that contains gradients for the corresponding tensor. Here are some more snippets on using Autograd & Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "# Do some operation\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "# Let’s compute the gradient now\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: When we wrap our Tensors with Variables, the arithmetic operations still remain the same, but Variables also remember their history of computation. Thus, `z` is not only a regular `2 x 2` Tensor but expression, involving `y` & `x`. This is what helps us define a **computational graph**. Nodes in this graph are Tensors & edges will be Functions operated on these nodes. **Backpropagating** through this graph then allows you to easily compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables and AutoGrad Exercise\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://alykhantejani.github.io/images/gradient_descent_line_graph.gif\"/>\n",
    "</p>\n",
    "\n",
    "So until now, we've seen Tensors that hold the data, Variables wrap around Tensors to let them perform complex math operations & finally `autograd` to compute gradients. So why do these Variables need to retain a history of computation?\n",
    "\n",
    "The reason we wish to retain a computational graph of these variables is so we can differentiate & update them to optimize mathematical equations. This may not make much sense now, but hang on for a while. We'll get there. Say we have two Variables `y_` & `y`. `y_` is what our model predicts & `y` is what it **should** predict (remember supervised learning?).\n",
    "\n",
    "But how do we teach a machine that it's not doing a very good job of predicting `y` & needs to do better? You see, the basis of learning, be it biological beings like us or artificial machines, has always been 'repetition' of a  particular task i.e. a **learning algorithm**. To achieve this, we optimize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.FloatTensor([3.0]), requires_grad=False)\n",
    "y_ = Variable(torch.FloatTensor([5.0]), requires_grad=True)\n",
    "w = Variable(torch.randn(torch.FloatTensor(1)), requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([y, y_], lr=0.1)\n",
    "for i in range(100):\n",
    "    error = (y_ - y*w).abs()   # Minimizes absolute difference\n",
    "    error.backward()      # Computes derivatives automatically\n",
    "    optimizer.step()     # Decreases loss: Updates y_ to become 'more' close to y\n",
    "    optimizer.zero_grad()\n",
    "print (y_)  # Evaluates to 3.0\n",
    "print (y)  # Evaluates to 3.0 -- optimization successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above snippet creates an optimizer called Stochastic Gradient Descent, passing it a list of parameters to optimize & a [learning rate](https://medium.com/@balamuralim.1993/importance-of-learning-rate-in-machine-learning-920a323fcbfb). We try to minimize the difference between `y_` & `y`, slowly. And after 100 steps, they become equal.\n",
    "\n",
    "We'll even use advanced optimizers like Adagrad & Adam when we get to Neural Nets. They're usually slower & more explanatory but are less likely to **overshoot** & thus, are used a lot. `torch.optim` module contains a number of these optimizers.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://2.bp.blogspot.com/-eW63YjSyuwY/V1QP3b9ZSmI/AAAAAAAAFeY/VcLfkmRvGaQbRjKhetlKjIl59kgkGV6hQCKgB/s1600/opt1.gif\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Trivia: Why are we doing this in PyTorch? Why not TensorFlow?\n",
    "\n",
    "You may skip this section & will still do fine, but it's interesting to know how exactly TensorFlow & PyTorch differ and how PyTorch is gaining so much popularity.\n",
    "\n",
    "With PyTorch & Tensorflow, being the two most comprehensive & popular frameworks, it didn't take much time to boil down our options to these two. Even though TensorFlow is more popular, we chose to go ahead with PyTorch for two primary reasons.\n",
    "\n",
    "1. **Graph Creation**: Creating & running graphs is where the two frameworks differ the most. Graphs in PyTorch are created dynamically, i.e at runtime. Whereas TensorFlow compiles the graph first, then executes it repeatedly. As a simple example, consider this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(T):\n",
    "    h = torch.matmul(W, h) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above operation takes place under a standard Python loop, `T` can be changed with each iteration of this code. TensorFlow on the other hand uses its [control flow operations](https://www.tensorflow.org/api_guides/python/control_flow_ops#Control_Flow_Operations) making it a bit too tedious to compute a graph dynamically. Furthermore, this makes debugging much easier. You'll see some more virtues of dynamic compute graphs in the upcoming articles.\n",
    "\n",
    "In TensorFlow, we define the [computate graph](https://www.tensorflow.org/programmers_guide/graphs) once and then execute the same graph over and over again, like a loop. In PyTorch, each forward pass defines a new computational graph.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.tensorflow.org/images/tensors_flowing.gif\"/>\n",
    "</p>\n",
    "\n",
    ">Credit: [TF Graph docs](https://www.tensorflow.org/programmers_guide/graphs)\n",
    "\n",
    ">Static graphs are nice because you can optimize the graph up front; framework might decide to fuse some graph\n",
    "operations for efficiency, or to come up with a strategy for distributing the graph across many GPUs or many\n",
    "machines. If you are reusing the same graph over and over, then this potentially costly up-front optimization can be amortized as the same graph is rerun over and over. However, for some models we may wish to perform\n",
    "different computations differently for each data point; for example a recurrent network might be unrolled for different numbers of time steps for each data point; this unrolling can be implemented as a loop. With a static graph the loop construct needs to be a part of the graph; for this reason TensorFlow provides operators such as tf.scan for embedding loops into the graph. With dynamic graphs the situation is simpler: since we build graphs on-the-fly for each example, we can use normal imperative flow control to perform computation that differs for each input.\n",
    "\n",
    "2. **Data Loaders**: With its well designed APIs, sampler & data loader, parallelizing data-flow operations is incredibly simple. TensorFlow provides us with some of its data loading tools (readers, queues, etc) but PyTorch is clearly miles ahead.\n",
    "\n",
    "*So why is TensorFlow so popular then?* While we may feel that learning about DL makes PyTorch a better candidate than TF, it may also be noted that there are certain fronts where TensorFlow does extremely well. Primarily in **Deployment**, **Device Management** & **Serialization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Exercise\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Exercise\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up: Handwritten Digit Classification\n",
    "\n",
    "So that's all for now. For the next article in this series, we are introducing a classical problem in Computer Vision: Handwritten Digit Recognition. Until now we've seen how to use Tensors (n-dimensional arrays) in PyTorch & compute their gradients with Autograd. The handwritten digit recognition is an example of a **classification** problem; given an image of a digit we can to classify it as either 0, 1, 2, 3...9. Each digit to be classified is known as a class. We will (try) to build a classifier with only whatever you've learned until now & then finally introduce you to the Artificial Neural Networks.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/sominwadhwa/sominwadhwa.github.io/blob/master/assets/intro_to_pytorch_series/mnist_logreg.jpeg?raw=true\"/>\n",
    "</p>\n",
    "\n",
    "Task: we'll be given a greyscale image (28 x 28) of some handwritten digit. We'll process this image to get a 28 x 28 matrix of real valued numbers, called **features** of this image. Our objective would be to **map a relationship between these features & the probability of a particular outcome**. Before moving on to the next article, if you are not familiar with this kind of a task, or wish to seek a quick intro to Logistic Regression, give [this article](https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389) a quick 5 minute read & you're good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For this task we will use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. We've already uploaded the entire [dataset on FloydHub](https://www.floydhub.com/redeipirati/datasets/mnist) & you can access the same via the `input` path.\n",
    "\n",
    "To learn how datasets are managed on FloydHub, you can checkout the [dataset documentation](https://docs.floydhub.com/guides/create_and_upload_dataset/) or checkout this quick [tutorial](https://blog.floydhub.com/getting-started-with-deep-learning-on-floydhub/).\n",
    "\n",
    "And that's all for now. You're ready to head over to the `ipython notebook` attached with this article & try some PyTorch exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "PyTorch provides an amazing framework with an awesome community that can support us in our DL journey. We introduced PyTorch & in the next article you'll some more traditional use cases of PyTorch; We'll be implementing a full scale `Classification` exercise on PyTorch using Logistic Regression, look for some improvements through a single layer Neural Network as well as create some more 'strange' networks to give you a good idea how Dynamic Compute graphs make PyTorch so powerful.\n",
    "\n",
    "*Note:* You should know that the PyTorch's [documentation](http://pytorch.org/docs/master/) and [tutorials](http://pytorch.org/tutorials/) are stored separately. And sometimes they may not converge due to the rapid speed of development and version changes. So feel free to investigate the [source code](https://github.com/pytorch/pytorch), if you feel so. [PyTorch Forums](https://discuss.pytorch.org/) are another great place to get your doubts cleared up. If you do however have any doubts/queries regarding our examples or in general, do let us know on the, we'll be happy to help.\n",
    "\n",
    "We hope you enjoyed this Introduction to PyTorch. If you'd like to share your feedback (cheers, bug fix, typo and/or improvements), please leave us a comment on our super active [forum](https://forum.floydhub.com/) or tweet us [@FloydHub_](https://twitter.com/FloydHub_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "**Big thanks** to:\n",
    " - [Illarion Khlestov](https://medium.com/@illarionkhlestov) for the code snippets & images.\n",
    " - [PyTorch](http://pytorch.org/tutorials/) for the docs, code snippets, images and the amazing framework.\n",
    " - [Justin Johnson](http://cs.stanford.edu/people/jcjohns/) for the pytorch examples and snippets of code.\n",
    "\n",
    "Link References:\n",
    " - Pytorch [docs](http://pytorch.org/docs/master/) and [tutorial](http://pytorch.org/tutorials/)\n",
    " - [jcjohnson pytorch examples](https://github.com/jcjohnson/pytorch-examples)\n",
    " - [PyTorch tutorial distilled by Illarion Khlestov](https://medium.com/towards-data-science/pytorch-tutorial-distilled-95ce8781a89c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

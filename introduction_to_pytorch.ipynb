{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FloydHub Introduction to Pytorch: a DL Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FloydHub handles a PyTorch image](images/FloydTorch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract\n",
    "\n",
    "[PyTorch](http://pytorch.org/) is an amazing [framework](https://en.wikipedia.org/wiki/Software_framework) which allows data scientists and AI practitioners to create amazing stuff. [Karpathy tweeted that this is the framework of 2017](https://twitter.com/karpathy/status/829518533772054529), [AI researchers are embracing it](https://www.oreilly.com/ideas/why-ai-and-machine-learning-researchers-are-beginning-to-embrace-pytorch) thanks to the high level of flexibility that the framework provide, moreover it’s pythonic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This introduction want to explore the magic behind PyTorch, with the strengths and weakness that the framework provide. Before we start, you should know that the Pytorch [documentation](http://pytorch.org/docs/master/) and [tutorials](http://pytorch.org/tutorials/) are stored separately. Also sometimes they may don’t meet each other, because of fast development and version changes. So feel free to investigate [source code](https://github.com/pytorch/pytorch). It’s very clear and straightforward. It’s good to mention that there are exist awesome [PyTorch forums](https://discuss.pytorch.org/), where you may ask any appropriate question, and you will get an answer relatively fast. This place seems to be even more popular than StackOverflow for the PyTorch users.\n",
    "\n",
    "**Table of Contents**:\n",
    "\n",
    "- Pytorch introduction\n",
    "- Tensor\n",
    "- Variables & Autograd\n",
    "- Defining new autograd functions\n",
    "- Static Vs Dynamic Computational Graph\n",
    "- Models Definition\n",
    "- Train model with CUDA\n",
    "- Weight Init\n",
    "- Excluding subGraph\n",
    "- Training Process\n",
    "- Logging\n",
    "- Data Handler\n",
    "- Final architecture overview\n",
    "- Summary\n",
    "\n",
    "*Note: During this introduction you will encounter ML/DL lingo and some training template with different models, even if you do not fully understand everything, don't worry, we will cover everything in a more concise way during the next episodes of this mini series.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch introduction\n",
    "\n",
    "PyTorch is a Python based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for numpy to use the power of GPUs\n",
    "- A deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "*This introduction assume that you have a basic familiarity of numpy, if it's not the case, follow this [link](https://cs231n.github.io/python-numpy-tutorial/#numpy) to a well done Numpy Tutorial authored by [Justin Johnson](http://cs.stanford.edu/people/jcjohns/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the package we need to run the tutorial\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import torchvision as tv\n",
    "\n",
    "# Is cuda available on this instance?\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "\n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of [50x or greater](https://github.com/jcjohnson/cnn-benchmarks), so unfortunately numpy won't be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Like numpy arrays, PyTorch Tensors do not know anything about deep learning or computational graphs or gradients; they are a generic tool for scientific computing.\n",
    "\n",
    "Here some example with PyTorch Tensor and some operations on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Tensor(5, 3):\n",
      "\n",
      " 0.0000e+00  0.0000e+00 -3.9585e-05\n",
      " 4.5654e-41 -3.9587e-05  4.5654e-41\n",
      "-1.2294e+02  4.5654e-41 -1.2294e+02\n",
      " 4.5654e-41  9.1393e+22  4.5654e-41\n",
      " 9.1398e+22  4.5654e-41  9.1388e+22\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "torch.rand(5, 3):\n",
      "\n",
      " 0.7432  0.9183  0.5229\n",
      " 0.7371  0.5490  0.8800\n",
      " 0.5685  0.4014  0.4555\n",
      " 0.6396  0.2774  0.3076\n",
      " 0.7764  0.1446  0.0406\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "Last Tensor Size:\n",
      "torch.Size([5, 3])\n",
      "Syntax 1: x + y =\n",
      "\n",
      " 1.0136  0.9723  1.1898\n",
      " 1.0181  1.5426  1.3739\n",
      " 1.2085  0.5547  1.3923\n",
      " 1.5246  0.9073  1.0915\n",
      " 0.9601  0.9105  0.6591\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "Syntax 2: torch.add(x, y) =\n",
      "\n",
      " 1.0136  0.9723  1.1898\n",
      " 1.0181  1.5426  1.3739\n",
      " 1.2085  0.5547  1.3923\n",
      " 1.5246  0.9073  1.0915\n",
      " 0.9601  0.9105  0.6591\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "Syntax 3: torch.add(x, y, out=result) =\n",
      "\n",
      " 1.0136  0.9723  1.1898\n",
      " 1.0181  1.5426  1.3739\n",
      " 1.2085  0.5547  1.3923\n",
      " 1.5246  0.9073  1.0915\n",
      " 0.9601  0.9105  0.6591\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "In-place Addition: y.add_(x) =\n",
      "\n",
      " 1.0136  0.9723  1.1898\n",
      " 1.0181  1.5426  1.3739\n",
      " 1.2085  0.5547  1.3923\n",
      " 1.5246  0.9073  1.0915\n",
      " 0.9601  0.9105  0.6591\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "Indexing x[:, 1] - Second column(index starts from zero) of every rows:\n",
      "\n",
      " 0.9183\n",
      " 0.5490\n",
      " 0.4014\n",
      " 0.2774\n",
      " 0.1446\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct a 5x3 matrix, uninitialized:\n",
    "print(\"torch.Tensor(5, 3):\")\n",
    "x = torch.Tensor(5, 3)\n",
    "print(x)\n",
    "\n",
    "# Construct a randomly initialized matrix\n",
    "print(\"torch.rand(5, 3):\")\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "# Get its size\n",
    "print(\"Last Tensor Size:\")\n",
    "print(x.size())\n",
    "\n",
    "# There are multiple syntaxes for operations. Let’s see addition as an example\n",
    "# Addition: syntax 1\n",
    "y = torch.rand(5, 3)\n",
    "print(\"Syntax 1: x + y =\")\n",
    "print(x + y)\n",
    "\n",
    "# Addition: syntax 2\n",
    "print(\"Syntax 2: torch.add(x, y) =\")\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# Addition: giving an output tensor\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(\"Syntax 3: torch.add(x, y, out=result) =\")\n",
    "print(result)\n",
    "\n",
    "# Addition: in-place\n",
    "# adds x to y\n",
    "print(\"In-place Addition: y.add_(x) =\")\n",
    "y.add_(x)\n",
    "print(y)\n",
    "\n",
    "# You can use standard numpy-like indexing with all bells and whistles!\n",
    "print (\"Indexing x[:, 1] - Second column(index starts from zero) of every rows:\")\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you simply need to cast it to a new datatype, but also, it very simple to switch from GPU to CPU. Moreover it’s very easy to convert tensors from NumPy to PyTorch and vice versa.\n",
    "\n",
    "Here we use PyTorch to convert a Numpy array to a PyTorch tensor and vice versa, then we will load a Tensor to GPU and then back again on CPU using cast before mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: you can run the GPU to CPU example only if you are running a FloydHub GPU instance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy tensor:  [[ 0.58664566  0.36939651 -0.57039737 -2.25794293]\n",
      " [ 0.92059809 -1.12255173  0.35163242  1.90652148]\n",
      " [ 1.91077947 -0.25575434  1.04416444  0.12377219]] \n",
      "\n",
      "Numpy to PyTorch tensor:  \n",
      " 0.5866  0.3694 -0.5704 -2.2579\n",
      " 0.9206 -1.1226  0.3516  1.9065\n",
      " 1.9108 -0.2558  1.0442  0.1238\n",
      "[torch.FloatTensor of size 3x4]\n",
      " \n",
      "\n",
      "PyTorch to Numpy tensor:  [[ 0.58664566  0.36939651 -0.57039737 -2.25794293]\n",
      " [ 0.92059809 -1.12255173  0.35163242  1.90652148]\n",
      " [ 1.91077947 -0.25575434  1.04416444  0.12377219]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a sample matrix of 3 rows and 4 columns \n",
    "# from a Normal Distribution with Mean 0 and Var 1 \n",
    "numpy_tensor = np.random.randn(3, 4)\n",
    "print (\"Numpy tensor: \", numpy_tensor, \"\\n\")\n",
    "\n",
    "# Convert numpy array to pytorch array\n",
    "pytorch_tensor = torch.Tensor(numpy_tensor)\n",
    "print (\"Numpy to PyTorch tensor: \", pytorch_tensor, \"\\n\")\n",
    "# Or another way\n",
    "pytorch_tensor = torch.from_numpy(numpy_tensor)\n",
    "\n",
    "# Convert torch tensor to numpy representation\n",
    "print (\"PyTorch to Numpy tensor: \", pytorch_tensor.numpy(), \"\\n\")\n",
    "\n",
    "# If cuda is available, run GPU-to-CPU and vice versa example\n",
    "if cuda:\n",
    "    # If we want to use tensor on GPU provide another type\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    gpu_tensor = torch.randn(10, 20).type(dtype)\n",
    "    # Or just call `cuda()` method\n",
    "    gpu_tensor = pytorch_tensor.cuda()\n",
    "    print (\"PyTorch cuda gpu_tensor \", gpu_tensor, \"\\n\")\n",
    "    # Call back to the CPU\n",
    "    cpu_tensor = gpu_tensor.cpu()\n",
    "    print (\"PyTorch cuda tensor to cpu_tensor, gpu_tensor.cpu() \", cpu_tensor, \"\\n\")\n",
    "\n",
    "# Define pytorch tensors\n",
    "x = torch.randn(10, 20)\n",
    "y = torch.ones(20, 5)\n",
    "# `@` mean matrix multiplication from python3.5, PEP-0465\n",
    "res = x @ y # Same as torch.matmul(x, y)\n",
    "\n",
    "# Get the shape\n",
    "res.shape  # torch.Size([10, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and AutoGrad\n",
    "\n",
    "Tensors are an awesome part of the PyTorch. But mainly all we want is to build some [neural networks](https://youtu.be/aircAruvnKk). What is about [backpropagation](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network)? Of course, we can manually implement it, but what is the reason? Thankfully [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) exists. To support it PyTorch provides variables to you.\n",
    "\n",
    "![autograd.Variable](http://pytorch.org/tutorials/_images/Variable.png)\n",
    "*Credit: [PyTorch Variable docs](http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)*\n",
    "\n",
    "Variables are wrappers above tensors. With them, we can build our [computational graph](https://colah.github.io/posts/2015-08-Backprop/), and compute gradients automatically later on. Every variable instance has two attributes: `.data` that contain initial tensor itself and `.grad` that will contain gradients for the corresponding tensor.\n",
    "\n",
    "When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "PyTorch Variables have the same API as PyTorch Tensors: (almost) any operation that you can perform on a Tensor also works on Variables; the difference is that using Variables defines a computational graph, allowing you to automatically compute gradients.\n",
    "\n",
    "Here we use PyTorch Variables and autograd: first on simple operations, second on forward and backward step for a linear model for a Regression Task with manual Gradient Descent update, and finally, we perform a some training steps using one hidden layer Neural Network with optimizer to show how to automatically update the weights during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "x + 2 = y, Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      " \n",
      "\n",
      "y was created as a result of an operation, so we have  <torch.autograd.function.AddConstantBackward object at 0x7f4465ec79a8> \n",
      "\n",
      "y * y * * 3 = z Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " \n",
      " mean(z),  Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "After backprop, x Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Var and Authograd example on simple operations\n",
    "\n",
    "# Create a Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(\"x\", x)\n",
    "\n",
    "# Make an op\n",
    "y = x + 2\n",
    "print(\"x + 2 = y,\", y, \"\\n\")\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn.\n",
    "print(\"y was created as a result of an operation, so we have \", y.grad_fn, \"\\n\")\n",
    "\n",
    "# More op\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(\"y * y * * 3 = z\", z, \"\\n\", \"mean(z), \", out)\n",
    "\n",
    "# Let’s backprop now \n",
    "out.backward()\n",
    "print(\"After backprop, x\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(sample)  \n",
      "-2.9718  1.7070 -0.4305 -2.2820  0.5237\n",
      " 0.0004 -1.2039  3.5283  0.4434  0.5848\n",
      " 0.8407  0.5510  0.3863  0.9124 -0.8410\n",
      " 1.2282 -1.8661  1.4146 -1.8781 -0.4674\n",
      "-0.7576  0.4215 -0.4827 -1.1198  0.3056\n",
      " 1.0386  0.5206 -0.5006  1.2182  0.2117\n",
      "-1.0613 -1.9441 -0.9596  0.5489 -0.9901\n",
      "-0.3826  1.5037  1.8267  0.5561  1.6445\n",
      " 0.4973 -1.5067  1.7661 -0.3569 -0.1713\n",
      " 0.4068 -0.4284 -1.1299  1.4274 -1.4027\n",
      "[torch.FloatTensor of size 10x5]\n",
      " \n",
      " Dataset(labels),  \n",
      " 1.4825\n",
      "-1.1559\n",
      " 1.6190\n",
      " 0.9581\n",
      " 0.7747\n",
      " 0.1940\n",
      " 0.1687\n",
      " 0.3061\n",
      " 1.0743\n",
      "-1.0327\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      " Type: <class 'torch.FloatTensor'> \n",
      "\n",
      "At the beginnig w grad is  None \n",
      "\n",
      "After one forward step, w grad  Variable containing:\n",
      " 2.0259\n",
      " 3.3964\n",
      "-5.3824\n",
      " 4.8143\n",
      "-0.1810\n",
      "[torch.FloatTensor of size 5x1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 5x1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Var and Authograd example on single forward and backward step with manual GD(Gradient Descent)\n",
    "# For reproducibility\n",
    "torch.manual_seed(1)\n",
    "# Define an dataset of 10 samples and 5 features\n",
    "x_tensor = torch.randn(10, 5)\n",
    "y_tensor = torch.randn(10, 1)\n",
    "# Create Variable wrapper around Tensor\n",
    "x = Variable(x_tensor, requires_grad=False)\n",
    "y = Variable(y_tensor, requires_grad=False)\n",
    "# Define some weights\n",
    "w = Variable(torch.randn(5, 1), requires_grad=True)\n",
    "\n",
    "# Get variable tensor\n",
    "print(\"Dataset(sample) \", x_tensor, \"\\n\", \\\n",
    "      \"Dataset(labels), \", y_tensor, \"\\n\", \\\n",
    "      \"Type:\", type(w.data), \"\\n\")  # torch.FloatTensor\n",
    "# Get variable gradient\n",
    "print(\"At the beginnig w grad is \", w.grad, \"\\n\")  # None\n",
    "\n",
    "# MSE(Mean Squared Error Loss)\n",
    "loss = torch.mean((y - x @ w) ** 2)\n",
    "\n",
    "# Calculate the gradients\n",
    "loss.backward()\n",
    "print(\"After one forward step, w grad \", w.grad)  # some gradients\n",
    "# Manually apply gradients - Gradient Descent Update\n",
    "w.data -= 0.01 * w.grad.data\n",
    "# Manually zero gradients after update\n",
    "w.grad.data.zero_() # Tensor of 5 x 1 of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2.5723326206207275 at step 0\n",
      "Loss 2.112910747528076 at step 1\n",
      "Loss 1.831189751625061 at step 2\n",
      "Loss 1.6449611186981201 at step 3\n",
      "Loss 1.5127955675125122 at step 4\n"
     ]
    }
   ],
   "source": [
    "# Var and Authograd example on a single hidden layer NN with SGD training for few steps\n",
    "# Remember: import torch.nn.functional as F\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Define an dataset of 10 samples and 10 features\n",
    "x = Variable(torch.randn(10, 10), requires_grad=False)\n",
    "y = Variable(torch.randn(10, 1), requires_grad=False)\n",
    "# Define some weights\n",
    "w1 = Variable(torch.randn(10, 5), requires_grad=True)\n",
    "w2 = Variable(torch.randn(5, 1), requires_grad=True)\n",
    "\n",
    "# Load your tensors on GPU if available\n",
    "if cuda:\n",
    "    x, y, w1, w2 = x.cuda(), y.cuda(), w1.cuda(), w2.cuda()\n",
    "\n",
    "# The lenght of the step we perform during GD\n",
    "learning_rate = 0.1\n",
    "# MSE(Mean Squared Error Loss)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "if cuda:\n",
    "    loss_fn.cuda()\n",
    "# Stocastic Gradient Descent Optimizer => w = w - lr * w.grads\n",
    "optimizer = torch.optim.SGD([w1, w2], lr=learning_rate)\n",
    "# Training Steps over full dataset\n",
    "for step in range(5):\n",
    "    # Hidden Layer\n",
    "    hidden = F.sigmoid(x @ w1)\n",
    "    # Model Output/ prediction\n",
    "    pred = hidden @ w2\n",
    "    \n",
    "    # From Loss, Update the weight to improve prediction\n",
    "    loss = loss_fn(pred, y)\n",
    "    if cuda:\n",
    "        loss = loss.cpu()\n",
    "    l = np.asscalar(loss.data.numpy())\n",
    "    print (\"Loss {l} at step {i}\".format(l=l, i=step))\n",
    "    # Manually zero all previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Calculate new gradients\n",
    "    loss.backward()\n",
    "    # Apply new gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the last example we have updated the weights automatically following these steps: Define an Optimizer, Compute the feedforward step, Zeroed the gradients Compute the Loss and BackProp(Compute the gradients with respect to Loss and Update the weights). But the main point that you should get from the last snippet: **we still should manually zero gradients before calculating new ones**. This is one of the core concepts of the PyTorch. Sometimes it may be not very obvious why we should do this, but on the other hand, we have full control over our gradients, when and how we want to apply them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining new autograd functions\n",
    " \n",
    "Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The *forward function* computes output Tensors from input Tensors. The *backward function* receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.\n",
    "\n",
    "In PyTorch we can easily define our own autograd operator by defining a subclass of torch.autograd.Function and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Variables containing input data.\n",
    "\n",
    "In this example we define our own custom autograd function for performing the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) nonlinearity, and use it with the code just above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 13.59540843963623 at step 0\n",
      "Loss 1.5918103456497192 at step 1\n",
      "Loss 1.1643922328948975 at step 2\n",
      "Loss 0.9619898796081543 at step 3\n",
      "Loss 0.8448194265365601 at step 4\n"
     ]
    }
   ],
   "source": [
    "# A one hidden layer NN with SGD training for few steps with ReLU(defined as autograd functions)\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  We can implement our own custom autograd Functions by subclassing\n",
    "  torch.autograd.Function and implementing the forward and backward passes\n",
    "  which operate on Tensors.\n",
    "  \"\"\"\n",
    "  def forward(self, input):\n",
    "    \"\"\"\n",
    "    In the forward pass we receive a Tensor containing the input and return a\n",
    "    Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "    backward pass using the save_for_backward method.\n",
    "    \"\"\"\n",
    "    self.save_for_backward(input)\n",
    "    return input.clamp(min=0)\n",
    "\n",
    "  def backward(self, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "    with respect to the output, and we need to compute the gradient of the loss\n",
    "    with respect to the input.\n",
    "    \"\"\"\n",
    "    input, = self.saved_tensors\n",
    "    grad_input = grad_output.clone()\n",
    "    grad_input[input < 0] = 0\n",
    "    return grad_input\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Define an dataset of 10 samples and 10 features\n",
    "x = Variable(torch.randn(10, 10), requires_grad=False)\n",
    "y = Variable(torch.randn(10, 1), requires_grad=False)\n",
    "# Define some weights\n",
    "w1 = Variable(torch.randn(10, 5), requires_grad=True)\n",
    "w2 = Variable(torch.randn(5, 1), requires_grad=True)\n",
    "\n",
    "# Load your tensors on GPU if available\n",
    "if cuda:\n",
    "    x, y, w1, w2 = x.cuda(), y.cuda(), w1.cuda(), w2.cuda()\n",
    "\n",
    "# The lenght of the step we perform during GD\n",
    "learning_rate = 0.1\n",
    "# MSE(Mean Squared Error Loss)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "if cuda:\n",
    "    loss_fn.cuda()\n",
    "# Stocastic Gradient Descent Optimizer => w = w - lr * w.grads\n",
    "optimizer = torch.optim.SGD([w1, w2], lr=learning_rate)\n",
    "# Training Steps over full dataset\n",
    "for step in range(5):\n",
    "    # Define our ReLU\n",
    "    relu = MyReLU()\n",
    "    # Hidden Layer\n",
    "    hidden = relu(x @ w1)\n",
    "    # Model Output/ prediction\n",
    "    pred = hidden @ w2\n",
    "    \n",
    "    # From Loss, Update the weight to improve prediction\n",
    "    loss = loss_fn(pred, y)\n",
    "    if cuda:\n",
    "        loss = loss.cpu()\n",
    "    l = np.asscalar(loss.data.numpy())\n",
    "    print (\"Loss {l} at step {i}\".format(l=l, i=step))\n",
    "    # Manually zero all previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Calculate new gradients\n",
    "    loss.backward()\n",
    "    # Apply new gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static vs Dynamic Computational Graph\n",
    " \n",
    "PyTorch autograd looks a lot like TensorFlow: in both frameworks we define a computational graph, and use automatic differentiation to compute gradients. The biggest difference between the two is that TensorFlow's computational graphs are **static** and PyTorch uses **dynamic** computational graphs.\n",
    "\n",
    "In TensorFlow, we define the [computational graph](https://www.tensorflow.org/programmers_guide/graphs) once and then execute the same graph over and over again, possibly feeding different input data to the graph. In PyTorch, each forward pass defines a new computational graph. In the beginning, the distinction between those approaches not so huge. But dynamic graphs became very handful when you want to debug your code or define some conditional statements. You can use your favorite debugger as it is!\n",
    "\n",
    "Static graphs are nice because you can optimize the graph up front; for example a framework might decide to fuse some graph operations for efficiency, or to come up with a strategy for distributing the graph across many GPUs or many machines. If you are reusing the same graph over and over, then this potentially costly up-front optimization can be amortized as the same graph is rerun over and over.\n",
    "\n",
    "![TF data flow](https://www.tensorflow.org/images/tensors_flowing.gif)\n",
    "*Credit: [TF Graph docs](https://www.tensorflow.org/programmers_guide/graphs)*\n",
    "\n",
    "One aspect where static and dynamic graphs differ is control flow. For some models we may wish to perform different computation for each data point; for example a recurrent network might be unrolled for different numbers of time steps for each data point; this unrolling can be implemented as a loop. With a static graph the loop construct needs to be a part of the graph; for this reason TensorFlow provides operators such as tf.scan for embedding loops into the graph. With dynamic graphs the situation is simpler: since we build graphs on-the-fly for each example, we can use normal imperative flow control to perform computation that differs for each input.\n",
    " \n",
    "Here's a comparison two definitions of the while loop statements - the first one in TensorFlow and the second one in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: We have already provisioning this machine with Tensorflow, declaring this dependency in the `floyd_requirement.txt` file. So every time you need to use a package that is not available in the environment you will use, just remember to add that dependecy in the `floyd_requirement.txt` file. You can also run commands directly from Jupyter Notebook. For more infos about how to install extra dependecies, just take a look at our docs [here](https://docs.floydhub.com/guides/jobs/installing_dependencies/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Loop example\n",
    "import tensorflow as tf\n",
    "\n",
    "#### Constant and Variable ####\n",
    "# Define the Variable and Constants we use in the computation\n",
    "first_counter = tf.constant(0)\n",
    "second_counter = tf.constant(10)\n",
    "some_value = tf.Variable(15)\n",
    "\n",
    "#### Computational Graph ####\n",
    "# We build the CG defining the ops to perform on the Tensors\n",
    "# Condition should handle all args:\n",
    "def cond(first_counter, second_counter, *args):\n",
    "    return first_counter < second_counter\n",
    "\n",
    "# Add Ops\n",
    "def body(first_counter, second_counter, some_value):\n",
    "    first_counter = tf.add(first_counter, 2)\n",
    "    second_counter = tf.add(second_counter, 1)\n",
    "    return first_counter, second_counter, some_value\n",
    "\n",
    "# Loop Op\n",
    "c1, c2, val = tf.while_loop(\n",
    "    cond, body, [first_counter, second_counter, some_value])\n",
    "\n",
    "#### Session ####\n",
    "# Where the execution takes place\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    counter_1_res, counter_2_res = sess.run([c1, c2])\n",
    "    print (counter_1_res, counter_2_res) # 20, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 20\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      " 20\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Loop example\n",
    "\n",
    "first_counter = torch.Tensor([0])\n",
    "second_counter = torch.Tensor([10])\n",
    "some_value = torch.Tensor(15)\n",
    "\n",
    "while (first_counter < second_counter)[0]:\n",
    "    first_counter += 2\n",
    "    second_counter += 1\n",
    "print (first_counter, second_counter) # 20, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code readability of PyTorch is really superior without any doubt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Definition\n",
    "\n",
    "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.\n",
    "\n",
    "When building neural networks we frequently think of arranging the computation into **layers**, some of which have **learnable parameters** which will be optimized during learning.\n",
    "\n",
    "In TensorFlow, packages like [Keras](https://github.com/fchollet/keras), [TensorFlow-Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim), and [TFLearn](http://tflearn.org/) provide higher-level abstractions over raw computational graphs that are useful for building neural networks.\n",
    "\n",
    "In PyTorch, the `nn` package serves this same purpose. The `nn` package defines a set of **Modules**, which are roughly equivalent to neural network layers. A Module receives input Variables and computes output Variables, but may also hold internal state such as Variables containing learnable parameters. The `nn` package also defines a set of useful loss functions that are commonly used when training neural networks.\n",
    "\n",
    "Do you remeber the one hidden layer NN the we used before? Now we will make the code `nn` compliant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1.3069813251495361 at step 0\n",
      "Loss 1.080486536026001 at step 1\n",
      "Loss 0.9181761741638184 at step 2\n",
      "Loss 0.7688356637954712 at step 3\n",
      "Loss 0.6261234879493713 at step 4\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Define an dataset of 10 samples and 10 features\n",
    "x = Variable(torch.randn(10, 10), requires_grad=False)\n",
    "y = Variable(torch.randn(10, 1), requires_grad=False)\n",
    "\n",
    "if cuda:\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Variables for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(10, 10),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(10, 1),\n",
    "        )\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# The lenght of the step we perform during GD\n",
    "learning_rate = 0.1\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "if cuda:\n",
    "    loss_fn.cuda()\n",
    "\n",
    "# Stocastic Gradient Descent Optimizer => w = w - lr * w.grads\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# Training Steps over full dataset\n",
    "for step in range(5):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Variable of input data to the Module and it produces\n",
    "    # a Variable of output data.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # From Loss, Update the weight to improve prediction\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if cuda:\n",
    "        loss = loss.cpu()\n",
    "    l = np.asscalar(loss.data.numpy())\n",
    "    print (\"Loss {l} at step {i}\".format(l=l, i=step))\n",
    "    # Manually zero all previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Variables with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "    # Apply new gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with CUDA\n",
    "\n",
    "If was discussed earlier how we might pass one tensor to [CUDA](https://en.wikipedia.org/wiki/CUDA). But if we want to pass the whole model, it’s ok to call `.cuda()` method from the model itself, and wrap each input variable to the `.cuda()` and it will be enough. After all computations, we should get results back with `.cpu()` method.\n",
    "```bash\n",
    "x = Variable(torch.randn(10, 10), requires_grad=False)\n",
    "y = Variable(torch.randn(10, 1), requires_grad=False)\n",
    "\n",
    "if cuda:\n",
    "    x, y = x.cuda(), y.cuda() <== CUDA Variable\n",
    "...\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(10, 10),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(10, 1),\n",
    "        )\n",
    "\n",
    "if cuda:\n",
    "    model.cuda() <== CUDA model\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "if cuda:\n",
    "    loss_fn.cuda() <= compute loss between cuda Tensor\n",
    "\n",
    "...\n",
    "# Inside Training\n",
    "\n",
    "if cuda:\n",
    "        loss = loss.cpu() <= From CUDA to CPU Tensor\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "In TensorFlow weights initialization mainly are made during tensor declaration. PyTorch offers another approach — at first, tensor should be declared, and on the next step weights for this tensor should be changed. Weights can be initialized as direct access to the tensor attribute, as a call to the bunch of methods inside `torch.nn.init` package. This decision can be not very straightforward, but it becomes useful when you want to initialize all layers of some type with same initialization.\n",
    "\n",
    "Here's some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 3 Ways to perform direct weights init ###\n",
    "\n",
    "# New way with `init` module\n",
    "w = torch.Tensor(3, 5)\n",
    "torch.nn.init.normal(w)\n",
    "\n",
    "# Work for Variables also\n",
    "w2 = Variable(w)\n",
    "torch.nn.init.normal(w2)\n",
    "\n",
    "# Old styled direct access to tensors data attribute\n",
    "w2.data.normal_()\n",
    "\n",
    "### Weights Init for Module ###\n",
    "# Example for some module\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    print (classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        print(\"Conv init\")\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        print(\"Batch init\")\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# If you use this approach run the follow steps:\n",
    "#\n",
    "# model = Model()\n",
    "# model.apply(weights_init)\n",
    "#\n",
    "# Follow the link for a full reference:\n",
    "# https://github.com/floydhub/dcgan/blob/master/main.py#L96-L142\n",
    "  \n",
    "    \n",
    "# For loop approach with direct access\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "# With the last approach the weights init will take place in the constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Excluding subgraphs from backward\n",
    "\n",
    "Sometimes when you want to retrain some layers of your model or prepare it for the production mode, it’s great when you can disable autograd mechanics for some layers. For this purposes, [PyTorch provides two flags](http://pytorch.org/docs/master/notes/autograd.html): `requires_grad` and `volatile`. First one will disable gradients for current layer, but child nodes still can calculate some. The second one will disable autograd for current layer and for all child nodes.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad False\n",
      "b.requires_grad True\n",
      "a.requires_grad False\n"
     ]
    }
   ],
   "source": [
    "# Requires grad\n",
    "# If there’s a single input to an operation that requires gradient,\n",
    "# Its output will also require gradient.\n",
    "x = Variable(torch.randn(5, 5))\n",
    "y = Variable(torch.randn(5, 5))\n",
    "z = Variable(torch.randn(5, 5), requires_grad=True)\n",
    "a = x + y\n",
    "print (\"a.requires_grad\", a.requires_grad)  # False\n",
    "b = a + z\n",
    "print (\"b.requires_grad\", b.requires_grad)  # True\n",
    "\n",
    "# Volatile differs from requires_grad in how the flag propagates.\n",
    "# If there’s even a single volatile input to an operation,\n",
    "# Its output is also going to be volatile.\n",
    "x = Variable(torch.randn(5, 5), requires_grad=True)\n",
    "y = Variable(torch.randn(5, 5), volatile=True)\n",
    "a = x + y\n",
    "print (\"a.requires_grad\", a.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process\n",
    "\n",
    "There are also some other bells and whistles in PyTorch. For example, you may use learning rate scheduler that will adjust your learning rate based on some rules. Or you may enable/disable batch norm layers and dropouts with single train flag. If you want it’s easy to change random seed separately for CPU and GPU.\n",
    "\n",
    "Here's pseudo-code:\n",
    "```python\n",
    "# scheduler example\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Training template for 100 epochs\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    train()\n",
    "    validate()\n",
    "\n",
    "# Train flag can be updated with boolean\n",
    "# to disable dropout and batch norm learning\n",
    "# execute train step\n",
    "model.train(True)\n",
    "# or inside the train function just run\n",
    "model.train()\n",
    "\n",
    "# run inference step\n",
    "model.train(False)\n",
    "# or inside the validate function just run\n",
    "model.eval()\n",
    "\n",
    "# CPU seed\n",
    "torch.manual_seed(42)\n",
    "# GPU seed\n",
    "torch.cuda.manual_seed_all(42)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you may print info about your model, or save/load it with few lines of code. If your model was initialized with [OrderedDict](https://docs.python.org/3/library/collections.html) or class-based model string representation will contain names of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential (\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU ()\n",
      "  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "    ('relu2', nn.ReLU())\n",
    "]))\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Sequential (\n",
    "#   (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
    "#   (relu1): ReLU ()\n",
    "#   (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
    "#   (relu2): ReLU ()\n",
    "# )\n",
    "\n",
    "save_path_params = 'model_params.ckp'\n",
    "\n",
    "# save/load only the model parameters(prefered solution)\n",
    "torch.save(model.state_dict(), save_path_params)\n",
    "model.load_state_dict(torch.load(save_path_params))\n",
    "\n",
    "save_path_model = 'model.ckp'\n",
    "# save whole model\n",
    "torch.save(model, save_path_model)\n",
    "model = torch.load(save_path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the saved file with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.ckp  model_params.ckp\r\n"
     ]
    }
   ],
   "source": [
    "!ls model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per PyTorch documentation saving model with `state_dict()` method is [more preferable](http://pytorch.org/docs/master/notes/serialization.html).\n",
    "\n",
    "*Note: If you want to load the model weights trained on GPU to CPU, use this: `torch.load('my_model.ckp', map_location=lambda storage, loc: storage)` according to [this thread on PyTorch Discussion](https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging\n",
    "\n",
    "Logging of the training process is a pretty important part. Unfortunately, PyTorch has no any tools like [tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard). So you may use usual text logs with [Python logging module](https://docs.python.org/3/library/logging.html) or try some of the third party libraries:\n",
    "\n",
    "- [A simple logger for experiments](https://github.com/oval-group/logger)\n",
    "- [A language-agnostic interface to TensorBoard](https://github.com/torrvision/crayon)\n",
    "- [Log TensorBoard events without touching TensorFlow](https://github.com/TeamHG-Memex/tensorboard_logger)\n",
    "- [Tensorboard for pytorch](https://github.com/lanpa/tensorboard-pytorch)\n",
    "- [Facebook visualization library wisdom](https://github.com/facebookresearch/visdom)\n",
    "- [Matplotlib](https://github.com/matplotlib/matplotlib)\n",
    "\n",
    "During the next episodes we will try to explore these different solution to let you have a wide choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling\n",
    "\n",
    "You may remember [data loaders proposed in TensorFlow](https://www.tensorflow.org/api_guides/python/reading_data) or even tried to implement some of them. This pipeline it's not very easy to understand, but it widely adopted.\n",
    "\n",
    "![Tf data loaders pipeline](https://cdn-images-1.medium.com/max/1280/1*S00VU2HiEjNZ35zlj2kqfw.gif)\n",
    "*Credit: [TF reading data docs](https://www.tensorflow.org/api_guides/python/reading_data)*\n",
    "\n",
    "PyTorch developers decided do not reinvent the wheel(a classical [anti-pattern](https://en.wikipedia.org/wiki/Anti-pattern) in sw Engineering). They just use multiprocessing. To create your own custom data loader, it’s enough to inherit your class from `torch.utils.data.Dataset` and change some methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Remeber: import torchivision as tv\n",
    "\n",
    "class ImagesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transform=None,\n",
    "                 loader=tv.datasets.folder.default_loader):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        target = row['class_']\n",
    "        path = row['path']\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        n, _ = self.df.shape\n",
    "        return n\n",
    "\n",
    "# what transformations should be done with our images\n",
    "data_transforms = tv.transforms.Compose([\n",
    "    tv.transforms.RandomCrop((64, 64), padding=4),\n",
    "    tv.transforms.RandomHorizontalFlip(),\n",
    "    tv.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_df = pd.read_csv('path/to/some.csv')\n",
    "# initialize our dataset at first\n",
    "train_dataset = ImagesDataset(\n",
    "    df=train_df,\n",
    "    transform=data_transforms\n",
    ")\n",
    "\n",
    "# initialize data loader with required number of workers and other params\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=10,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=16)\n",
    "\n",
    "# fetch the batch(call to `__getitem__` method)\n",
    "for img, target in train_loader:\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the mini series we will have lot of fun and time for playing with Dataset and DataLoader :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two things you should know. First — image dimensions are different from TensorFlow. They are [batch_size x channels x height x width]. But this transformation can be made without you interaction by preprocessing step `torchvision.transforms.ToTensor()`. There are also a lot of useful utils in the [transforms package](http://pytorch.org/docs/master/torchvision/transforms.html).\n",
    "\n",
    "The second important thing that you may use pinned memory on GPU. For this, you just need to place additional flag `async=Tru`e to a `cuda()` call and get pinned batches from DataLoader with flag `pin_memory=True`. More about this feature discussed [here](http://pytorch.org/docs/master/notes/cuda.html#use-pinned-memory-buffers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final architecture overview\n",
    "\n",
    "Now you know about models, optimizers and a lot of other stuff. What is the right way to merge all of them? I propose to split your models and all wrappers on such building blocks:\n",
    "\n",
    "![Summary](https://cdn-images-1.medium.com/max/1280/1*A-cWYNur2lqDEhUF1_gdCw.png)\n",
    "\n",
    "And here is pseudo-code template for a ML/DL script:\n",
    "\n",
    "```python\n",
    "class ImagesDataset(torch.utils.data.Dataset):\n",
    "    pass\n",
    "\n",
    "class Net(nn.Module):\n",
    "    pass\n",
    "\n",
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "dataset = ImagesDataset(path_to_images)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "\n",
    "train = True\n",
    "for epoch in range(epochs):\n",
    "    if train:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = Variable(to_gpu(inputs))\n",
    "        labels = Variable(to_gpu(labels))\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if not train: \n",
    "        save_best_model(epoch_validation_accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "PyTorch is an amazing framework from which starts a DL journey. Numpy extension, PyThonic, Flexible, easy to understand and debug. Already widely adopted by AI research community(FAIR is widely adopting it and NVIDIA is offering full support), unfortunately it’s not production ready as TF, it lack of a proper monitoring solution and it’s not widely available in term of blog/video resources as TF but we are sure that at the end of the Beta it will become mainstream. \n",
    "\n",
    "PyTorch provides an amazing framwork with an awesome community that can support us in our DL journey.\n",
    "\n",
    "If you have enjoied this Introduction, or you want to share your feedback(cheers, bug fix, typo and/or improvements), please leave a comment on our super active Forum webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks and Resources\n",
    "\n",
    "**Big thanks** to: \n",
    " - [Illarion Khlestov](https://medium.com/@illarionkhlestov) for the code snippets, image and article, \n",
    " - [PyTorch](http://pytorch.org/tutorials/) for the docs, code snippet, image and the amazing framework\n",
    " - [Justin Johnson](http://cs.stanford.edu/people/jcjohns/) for the pytorch examples and snippet of code\n",
    "\n",
    "Link References:\n",
    " - Pytorch [docs](http://pytorch.org/docs/master/) and [tutorial](http://pytorch.org/tutorials/)\n",
    " - [jcjohnson pytorch examples](https://github.com/jcjohnson/pytorch-examples)\n",
    " - [PyTorch tutorial distilled by Illarion Khlestov](https://medium.com/towards-data-science/pytorch-tutorial-distilled-95ce8781a89c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
